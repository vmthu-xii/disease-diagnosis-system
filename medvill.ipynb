{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-14T02:31:00.856161Z","iopub.status.busy":"2024-05-14T02:31:00.855071Z","iopub.status.idle":"2024-05-14T02:31:16.347104Z","shell.execute_reply":"2024-05-14T02:31:16.345652Z","shell.execute_reply.started":"2024-05-14T02:31:00.856102Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pytorch-pretrained-bert in /opt/conda/lib/python3.10/site-packages (0.6.2)\n","Requirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (2.1.2)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (1.26.4)\n","Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (1.26.100)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (2.31.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (4.66.1)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from pytorch-pretrained-bert) (2023.12.25)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2024.2.0)\n","Requirement already satisfied: botocore<1.30.0,>=1.29.100 in /opt/conda/lib/python3.10/site-packages (from boto3->pytorch-pretrained-bert) (1.29.165)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3->pytorch-pretrained-bert) (0.6.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-pretrained-bert) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-pretrained-bert) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-pretrained-bert) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pytorch-pretrained-bert) (2024.2.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3->pytorch-pretrained-bert) (2.9.0.post0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.100->boto3->pytorch-pretrained-bert) (1.16.0)\n"]}],"source":["!pip install pytorch-pretrained-bert\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T02:31:16.350337Z","iopub.status.busy":"2024-05-14T02:31:16.349857Z","iopub.status.idle":"2024-05-14T02:31:29.925737Z","shell.execute_reply":"2024-05-14T02:31:29.924671Z","shell.execute_reply.started":"2024-05-14T02:31:16.350280Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\n"]}],"source":["!pip install tqdm"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T02:31:29.927601Z","iopub.status.busy":"2024-05-14T02:31:29.927277Z","iopub.status.idle":"2024-05-14T02:31:38.196503Z","shell.execute_reply":"2024-05-14T02:31:38.195543Z","shell.execute_reply.started":"2024-05-14T02:31:29.927573Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","from torchvision.transforms import ToTensor\n","from PIL import Image\n","import torch.nn.functional as F\n","from transformers import BertConfig, BertModel, BertPreTrainedModel, AutoConfig\n","\n","\n","import torch\n","import torch.nn as nn\n","from transformers import BertConfig, BertModel\n","import functools\n","import json\n","import os\n","from collections import Counter\n","\n","import torch\n","import torchvision.transforms as transforms\n","from pytorch_pretrained_bert import BertTokenizer, BertAdam\n","import json\n","import numpy as np\n","import os\n","from PIL import Image\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","import contextlib\n","\n","import random\n","import torch.optim as optim\n","import tqdm\n","from sklearn.metrics import f1_score, accuracy_score, roc_auc_score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# DATA"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T02:31:38.199483Z","iopub.status.busy":"2024-05-14T02:31:38.198975Z","iopub.status.idle":"2024-05-14T02:31:38.208712Z","shell.execute_reply":"2024-05-14T02:31:38.207682Z","shell.execute_reply.started":"2024-05-14T02:31:38.199452Z"},"trusted":true},"outputs":[],"source":["def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","set_seed(0)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T02:31:38.210206Z","iopub.status.busy":"2024-05-14T02:31:38.209877Z","iopub.status.idle":"2024-05-14T02:31:38.221460Z","shell.execute_reply":"2024-05-14T02:31:38.220227Z","shell.execute_reply.started":"2024-05-14T02:31:38.210178Z"},"trusted":true},"outputs":[],"source":["class Args:\n","    def __init__(self):\n","        self.seed = 123\n","        self.batch_sz = 8\n","        self.max_epochs = 20\n","        self.task_type = \"multilabel\"\n","        self.n_workers = 4\n","        self.patience = 20\n","        \n","        output_path = 'output'\n","        self.savedir = \"/kaggle/working/\"\n","        self.save_name = 'mimic_par'\n","        \n","        self.loaddir = 'path/to/pre-trained_model'\n","        self.name = \"scenario_name\"\n","        \n","        self.openi = False\n","        self.data_path = '/kaggle/input/adip-hcmus-mimiccxr/'\n","        self.Train_dset_name = 'data/csv/train.jsonl'\n","        self.Valid_dset_name = 'data/csv/valid.jsonl'\n","        \n","        self.embed_sz = 768\n","        self.hidden_sz = 768\n","        self.bert_model = \"bert-base-uncased\"\n","        self.init_model = \"bert-base-uncased\"\n","        \n","        self.drop_img_percent = 0.0\n","        self.dropout = 0.1\n","        \n","        self.freeze_img = 0\n","        self.freeze_txt = 0\n","        \n","        self.freeze_img_all = False\n","        self.freeze_txt_all = False\n","        \n","        self.glove_path = \"/path/to/glove_embeds/glove.840B.300d.txt\"\n","        self.gradient_accumulation_steps = 2\n","        self.hidden = []\n","        \n","        self.img_embed_pool_type = \"avg\"\n","        self.img_hidden_sz = 2048\n","        self.include_bn = True\n","        \n","        self.lr = 1e-3\n","        self.lr_factor = 0.75\n","        self.lr_patience = 5\n","        \n","        self.max_seq_len = 512\n","        self.num_image_embeds = 256\n","        \n","        self.warmup = 0.1\n","        self.weight_classes = 1\n","    \n","args = Args()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T02:31:38.223130Z","iopub.status.busy":"2024-05-14T02:31:38.222645Z","iopub.status.idle":"2024-05-14T02:31:38.233096Z","shell.execute_reply":"2024-05-14T02:31:38.231653Z","shell.execute_reply.started":"2024-05-14T02:31:38.223098Z"},"trusted":true},"outputs":[],"source":["class Vocab(object):\n","    def __init__(self, emptyInit=False):\n","        if emptyInit:\n","            self.stoi, self.itos, self.vocab_sz = {}, [], 0\n","        else:\n","            self.stoi = {\n","                w: i\n","                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n","            }\n","            self.itos = [w for w in self.stoi]\n","            self.vocab_sz = len(self.itos)\n","\n","    def add(self, words):\n","        cnt = len(self.itos)\n","        for w in words:\n","            if w in self.stoi:\n","                continue\n","            self.stoi[w] = cnt\n","            self.itos.append(w)\n","            cnt += 1\n","        self.vocab_sz = len(self.itos)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T02:31:38.234625Z","iopub.status.busy":"2024-05-14T02:31:38.234324Z","iopub.status.idle":"2024-05-14T02:31:38.244815Z","shell.execute_reply":"2024-05-14T02:31:38.243752Z","shell.execute_reply.started":"2024-05-14T02:31:38.234597Z"},"trusted":true},"outputs":[],"source":["@contextlib.contextmanager\n","def numpy_seed(seed, *addl_seeds):\n","    \"\"\"Context manager which seeds the NumPy PRNG with the specified seed and\n","    restores the state afterward\"\"\"\n","    if seed is None:\n","        yield       # Không làm gì và kết thúc ngay lập tức\n","        return\n","    if len(addl_seeds) > 0:\n","        seed = int(hash((seed, *addl_seeds)) % 1e6)\n","    state = np.random.get_state()\n","    np.random.seed(seed)\n","    try:\n","        yield\n","    finally:\n","        np.random.set_state(state)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T02:31:38.246732Z","iopub.status.busy":"2024-05-14T02:31:38.246355Z","iopub.status.idle":"2024-05-14T02:31:38.264643Z","shell.execute_reply":"2024-05-14T02:31:38.263559Z","shell.execute_reply.started":"2024-05-14T02:31:38.246695Z"},"trusted":true},"outputs":[],"source":["class JsonlDataset(Dataset):\n","    def __init__(self, data_path, tokenizer, transforms, vocab, args, is_train=True):\n","        if is_train:\n","            with open(data_path + args.Train_dset_name, \"r\") as file:\n","                self.data = json.load(file)\n","        else:\n","            with open(data_path + args.Valid_dset_name, \"r\") as file:\n","                self.data = json.load(file)\n","        self.data_dir = os.path.dirname(data_path)\n","        self.tokenizer = tokenizer\n","        self.args = args\n","        self.vocab = vocab\n","        self.n_classes = len(args.labels)\n","        self.text_start_token = [\"[SEP]\"]\n","\n","        with numpy_seed(0):\n","            for row in self.data:\n","                if np.random.random() < args.drop_img_percent:\n","                    row[\"img\"] = None\n","\n","        self.max_seq_len = args.max_seq_len\n","        self.max_seq_len -= args.num_image_embeds\n","\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        sentence = (\n","            self.text_start_token\n","            + self.tokenizer(self.data[index][\"text\"])[: (self.max_seq_len - 1)]\n","            + self.text_start_token\n","        )\n","        segment = torch.zeros(len(sentence))\n","        sentence = torch.LongTensor(\n","            [\n","                self.vocab.stoi[w] if w in self.vocab.stoi else self.vocab.stoi[\"[UNK]\"]\n","                for w in sentence\n","            ]\n","        )\n","        if self.args.task_type == \"multilabel\":\n","            label = torch.zeros(self.n_classes)\n","            if self.data[index][\"label\"] == '':\n","                self.data[index][\"label\"] = \"'Others'\"\n","            else:\n","                pass\n","            label[  # Vector 14 chiều\n","                [self.args.labels.index(tgt) for tgt in self.data[index][\"label\"]]\n","            ] = 1\n","        else:\n","            pass\n","\n","        image = None\n","        if self.data[index][\"img\"]:\n","            image = Image.open(\n","                os.path.join(self.data_dir, self.data[index][\"img\"])).convert(\"RGB\")\n","        else:\n","            image = Image.fromarray(128 * np.ones((256, 256, 3), dtype=np.uint8))\n","        image = self.transforms(image)\n","\n","        # The first SEP is part of Image Token.\n","        segment = segment[1:]\n","        sentence = sentence[1:]\n","        # The first segment (0) is of images.\n","        segment += 1\n","\n","        return sentence, segment, image, label"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T02:31:38.266124Z","iopub.status.busy":"2024-05-14T02:31:38.265771Z","iopub.status.idle":"2024-05-14T02:31:38.276606Z","shell.execute_reply":"2024-05-14T02:31:38.275488Z","shell.execute_reply.started":"2024-05-14T02:31:38.266094Z"},"trusted":true},"outputs":[],"source":["def get_transforms(args):\n","    if args.openi:\n","        return transforms.Compose(\n","            [\n","                transforms.Grayscale(num_output_channels=3),\n","                transforms.ToTensor(),\n","                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","            ])\n","    else:\n","        return transforms.Compose(\n","            [\n","                transforms.ToTensor(),\n","                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","            ]\n","    )"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T02:31:38.281313Z","iopub.status.busy":"2024-05-14T02:31:38.280977Z","iopub.status.idle":"2024-05-14T02:31:38.288309Z","shell.execute_reply":"2024-05-14T02:31:38.287395Z","shell.execute_reply.started":"2024-05-14T02:31:38.281285Z"},"trusted":true},"outputs":[],"source":["def get_labels_and_frequencies(path):\n","    # Khởi tạo một Counter để đếm tần suất xuất hiện của các nhãn\n","    label_freqs = Counter()\n","    with open(path, \"r\") as file:\n","        data = json.load(file)\n","    data_labels = [line[\"label\"] for line in data]\n","    if type(data_labels) == list:\n","        for label_row in data_labels:\n","            if label_row == '':\n","                label_row = [\"'Others'\"]\n","\n","            label_freqs.update(label_row)\n","    else:\n","        pass\n","    return list(label_freqs.keys()), label_freqs"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T02:31:38.290352Z","iopub.status.busy":"2024-05-14T02:31:38.289781Z","iopub.status.idle":"2024-05-14T02:31:38.298825Z","shell.execute_reply":"2024-05-14T02:31:38.297858Z","shell.execute_reply.started":"2024-05-14T02:31:38.290298Z"},"trusted":true},"outputs":[],"source":["def get_vocab(args):\n","    vocab = Vocab()\n","    bert_tokenizer = BertTokenizer.from_pretrained(\n","        args.bert_model, do_lower_case=True\n","    )\n","    vocab.stoi = bert_tokenizer.vocab\n","    vocab.itos = bert_tokenizer.ids_to_tokens\n","    vocab.vocab_sz = len(vocab.itos)\n","\n","    return vocab"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T02:31:38.300946Z","iopub.status.busy":"2024-05-14T02:31:38.300263Z","iopub.status.idle":"2024-05-14T02:31:38.310506Z","shell.execute_reply":"2024-05-14T02:31:38.309551Z","shell.execute_reply.started":"2024-05-14T02:31:38.300908Z"},"trusted":true},"outputs":[],"source":["def collate_fn(batch, args):\n","    lens = [len(row[0]) for row in batch]\n","    bsz, max_seq_len = len(batch), max(lens)\n","\n","    mask_tensor = torch.zeros(bsz, max_seq_len).long()\n","    text_tensor = torch.zeros(bsz, max_seq_len).long()\n","    segment_tensor = torch.zeros(bsz, max_seq_len).long()\n","\n","    img_tensor = None\n","    img_tensor = torch.stack([row[2] for row in batch])\n","\n","    if args.task_type == \"multilabel\":\n","        # Multilabel case\n","        tgt_tensor = torch.stack([row[3] for row in batch])\n","    else:\n","        # Single Label case\n","        tgt_tensor = torch.cat([row[3] for row in batch]).long()\n","\n","    for i_batch, (input_row, length) in enumerate(zip(batch, lens)):\n","        tokens, segment = input_row[:2]\n","        text_tensor[i_batch, :length] = tokens\n","        segment_tensor[i_batch, :length] = segment\n","        mask_tensor[i_batch, :length] = 1\n","\n","    return text_tensor, segment_tensor, mask_tensor, img_tensor, tgt_tensor"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T02:31:38.312193Z","iopub.status.busy":"2024-05-14T02:31:38.311809Z","iopub.status.idle":"2024-05-14T02:31:38.322453Z","shell.execute_reply":"2024-05-14T02:31:38.321469Z","shell.execute_reply.started":"2024-05-14T02:31:38.312152Z"},"trusted":true},"outputs":[],"source":["def get_data_loaders(args):\n","    tokenizer = (\n","        BertTokenizer.from_pretrained(args.bert_model, do_lower_case=True).tokenize)\n","\n","    transforms = get_transforms(args)\n","\n","    args.labels, args.label_freqs = get_labels_and_frequencies(\n","        os.path.join(args.data_path, args.Train_dset_name)\n","    )\n","\n","    vocab = get_vocab(args)\n","    args.vocab = vocab\n","    args.vocab_sz = vocab.vocab_sz\n","    args.n_classes = len(args.labels)\n","\n","    train_dataset = JsonlDataset(\n","        os.path.join(args.data_path),\n","        tokenizer,\n","        transforms,\n","        vocab,\n","        args,\n","    )\n","\n","    args.train_data_len = len(train_dataset)\n","\n","    val_dataset = JsonlDataset(\n","        os.path.join(args.data_path),\n","        tokenizer,\n","        transforms,\n","        vocab,\n","        args,\n","        is_train=False,\n","    )\n","\n","    collate = functools.partial(collate_fn, args=args)\n","\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=args.batch_sz,\n","        shuffle=False,\n","        num_workers=args.n_workers,\n","        collate_fn=collate,\n","    )\n","\n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=args.batch_sz,\n","        shuffle=False,\n","        num_workers=args.n_workers,\n","        collate_fn=collate,\n","    )\n","\n","    return train_dataset, train_loader, val_dataset, val_loader  # , test\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T02:31:38.324122Z","iopub.status.busy":"2024-05-14T02:31:38.323729Z","iopub.status.idle":"2024-05-14T02:31:38.750629Z","shell.execute_reply":"2024-05-14T02:31:38.749646Z","shell.execute_reply.started":"2024-05-14T02:31:38.324085Z"},"trusted":true},"outputs":[],"source":["train_dataset, train_dataloader, val_dataset, val_dataloader = get_data_loaders(args)"]},{"cell_type":"markdown","metadata":{},"source":["# MODEL"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T02:31:38.752446Z","iopub.status.busy":"2024-05-14T02:31:38.752014Z","iopub.status.idle":"2024-05-14T02:31:38.757419Z","shell.execute_reply":"2024-05-14T02:31:38.756241Z","shell.execute_reply.started":"2024-05-14T02:31:38.752405Z"},"trusted":true},"outputs":[],"source":["# args = {\n","#     \"img_hidden_sz\" : 2048,\n","#     \"hidden_sz\" : 768,\n","#     \"dropout\" : 0.1 ,\n","#     \"num_image_embeds\" : 256,\n","#     \"init_model\" : \"bert-base-uncased\",\n","#     \"n_classes\" : 14,\n","#     \"img_embed_pool_type\" : \"avg\" ,\n","#     \"vocab\" : []\n","# }"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T03:01:56.715808Z","iopub.status.busy":"2024-05-14T03:01:56.715401Z","iopub.status.idle":"2024-05-14T03:01:56.724880Z","shell.execute_reply":"2024-05-14T03:01:56.723627Z","shell.execute_reply.started":"2024-05-14T03:01:56.715774Z"},"trusted":true},"outputs":[],"source":["class ImageEncoder(nn.Module):\n","    def __init__(self, args):\n","        super(ImageEncoder, self).__init__()\n","        self.args = args\n","        model = torchvision.models.resnet50(pretrained=True)\n","        modules = list(model.children())[:-2]\n","        self.model = nn.Sequential(*modules)\n","\n","        pool_func = (\n","            nn.AdaptiveAvgPool2d\n","            if args.img_embed_pool_type == \"avg\"\n","            else nn.AdaptiveMaxPool2d\n","        )\n","\n","\n","    def forward(self, x):\n","        # Bx3x224x224 -> Bx2048x7x7 -> Bx2048xN -> BxNx2048\n","\n","        # out = self.pool(self.model(x))\n","        # out = torch.flatten(out, start_dim=2)\n","        # out = out.transpose(1, 2).contiguous()\n","        \n","        out = self.model(x)\n","        out = torch.flatten(out, start_dim=2) #out torch.Size([100, 2048, 3])\n","        out = out.transpose(1, 2).contiguous() #out torch.Size([100, 3, 2048])\n","\n","        # print(\"out.size()\",out.size())\n","        # input(\"STOP!!!\")\n","        \n","\n","        return out  # BxNx2048\n","\n"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T03:01:57.008406Z","iopub.status.busy":"2024-05-14T03:01:57.008001Z","iopub.status.idle":"2024-05-14T03:01:57.020982Z","shell.execute_reply":"2024-05-14T03:01:57.019884Z","shell.execute_reply.started":"2024-05-14T03:01:57.008374Z"},"trusted":true},"outputs":[],"source":["class ImageBertEmbeddings(nn.Module):\n","    def __init__(self, args, embeddings):\n","        super(ImageBertEmbeddings, self).__init__()\n","        self.args = args\n","        print(args)\n","        self.img_embeddings = nn.Linear(args.img_hidden_sz, args.hidden_sz)\n","        self.position_embeddings = embeddings.position_embeddings\n","        self.token_type_embeddings = embeddings.token_type_embeddings\n","        self.word_embeddings = embeddings.word_embeddings\n","        self.LayerNorm = embeddings.LayerNorm\n","        self.dropout = nn.Dropout(p=args.dropout)\n","        \n","    def forward(self, input_imgs, token_type_ids):\n","        bsz = input_imgs.size(0)\n","        seq_length = self.args.num_image_embeds + 2  # +2 for CLS and SEP Token\n","\n","        cls_id = torch.LongTensor([self.args.vocab.stoi[\"[CLS]\"]]).cuda()\n","        cls_id = cls_id.unsqueeze(0).expand(bsz, 1)\n","        cls_token_embeds = self.word_embeddings(cls_id)\n","\n","        sep_id = torch.LongTensor([self.args.vocab.stoi[\"[SEP]\"]]).cuda()\n","        sep_id = sep_id.unsqueeze(0).expand(bsz, 1)\n","        sep_token_embeds = self.word_embeddings(sep_id)\n","        \n","        #print(input_imgs.shape)\n","        imgs_embeddings = self.img_embeddings(input_imgs)\n","        print(imgs_embeddings)\n","        token_embeddings = torch.cat(\n","            [cls_token_embeds, imgs_embeddings, sep_token_embeds], dim=1)\n","        #print(imgs_embeddings.shape)\n","        \n","        position_ids = torch.arange(seq_length, dtype=torch.long).cuda()\n","        position_ids = position_ids.unsqueeze(0).expand(bsz, seq_length)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n","        #print(args.hidden_sz)\n","        embeddings = token_embeddings + position_embeddings + token_type_embeddings\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        return embeddings\n"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T03:01:57.423160Z","iopub.status.busy":"2024-05-14T03:01:57.421976Z","iopub.status.idle":"2024-05-14T03:01:57.435896Z","shell.execute_reply":"2024-05-14T03:01:57.434776Z","shell.execute_reply.started":"2024-05-14T03:01:57.423113Z"},"trusted":true},"outputs":[],"source":["class MultimodalBertEncoder(BertPreTrainedModel):\n","    def __init__(self, model_config, args, configs):\n","        super().__init__(model_config)\n","        self.args = args\n","        self.configs = configs\n","        bert = BertModel(model_config)\n","        \n","        self.txt_embeddings = bert.embeddings\n","        self.img_embeddings = ImageBertEmbeddings(args, self.txt_embeddings)\n","        self.img_encoder = ImageEncoder(args)\n","        self.encoder = bert.encoder\n","        self.pooler = bert.pooler\n","        #self.clf = nn.Linear(args.hidden_sz, args.n_classes)\n","\n","    def forward(self, input_txt, attention_mask, segment, input_img):\n","        bsz = input_txt.size(0)\n","        attention_mask = torch.cat(\n","            [\n","                torch.ones(bsz, self.args.num_image_embeds + 2).long().cuda(),\n","                attention_mask,\n","            ],\n","            dim=1)\n","        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n","\n","        try:\n","            extended_attention_mask = extended_attention_mask.to(\n","                dtype=next(self.parameters()).dtype)  # fp16 compatibility\n","        except StopIteration:\n","            extended_attention_mask = extended_attention_mask.to(dtype=torch.float16)\n","\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        img_tok = (\n","            torch.LongTensor(input_txt.size(0), self.args.num_image_embeds + 2)\n","            .fill_(0)\n","            .cuda())\n","        img = self.img_encoder(input_img)  # BxNx3x224x224 -> BxNx2048\n","\n","        \n","        img_embed_out = self.img_embeddings(img, img_tok)\n","        txt_embed_out = self.txt_embeddings(input_txt, segment)\n","        encoder_input = torch.cat([img_embed_out, txt_embed_out], 1)  # Bx(TEXT+IMG)xHID\n","        encoded_layers = self.encoder(encoder_input, extended_attention_mask)\n","        return self.pooler(encoded_layers[-1])"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T03:01:58.000226Z","iopub.status.busy":"2024-05-14T03:01:57.999837Z","iopub.status.idle":"2024-05-14T03:01:58.007248Z","shell.execute_reply":"2024-05-14T03:01:58.006093Z","shell.execute_reply.started":"2024-05-14T03:01:58.000193Z"},"trusted":true},"outputs":[],"source":["class MultimodalBertClf(BertPreTrainedModel):\n","    def __init__(self, model_config, args, configs):\n","        super().__init__(model_config)\n","        self.enc = MultimodalBertEncoder(model_config, args, configs)\n","        self.fc = nn.Linear(args.hidden_sz, args.n_classes)\n","\n","    def forward(self, txt, mask, segment, img):\n","        x = self.enc(txt, mask, segment, img)\n","        return self.fc(x)\n","    "]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T03:02:02.968701Z","iopub.status.busy":"2024-05-14T03:02:02.967971Z","iopub.status.idle":"2024-05-14T03:02:02.974249Z","shell.execute_reply":"2024-05-14T03:02:02.973100Z","shell.execute_reply.started":"2024-05-14T03:02:02.968666Z"},"trusted":true},"outputs":[],"source":["# Kiểm tra xem có GPU có sẵn hay không\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')  # Sử dụng GPU\n","else:\n","    device = torch.device('cpu')  # Sử dụng CPU\n"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T00:24:24.251511Z","iopub.status.busy":"2024-05-14T00:24:24.251125Z","iopub.status.idle":"2024-05-14T00:24:32.455785Z","shell.execute_reply":"2024-05-14T00:24:32.454870Z","shell.execute_reply.started":"2024-05-14T00:24:24.251481Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"]},{"name":"stdout","output_type":"stream","text":["<__main__.Args object at 0x7842e6ada110>\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 97.8M/97.8M [00:00<00:00, 132MB/s] \n","Some weights of MultimodalBertClf were not initialized from the model checkpoint at /kaggle/input/medvill-weight and are newly initialized: ['fc.bias', 'fc.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["{'fc.bias', 'fc.weight', 'enc.img_embeddings.word_embeddings.weight'}\n"]}],"source":["# Load the model state dict\n","model_state_dict = torch.load(\"/kaggle/input/medvill-weight/pytorch_model.bin\")\n","\n","model_config = AutoConfig.from_pretrained(\"/kaggle/input/medvill-weight\")  \n","model = MultimodalBertClf.from_pretrained(\"/kaggle/input/medvill-weight\", state_dict=model_state_dict,\n","                     args=args, configs=model_config, local_files_only=True).to(device)\n","\n","# Get the state dict of the model after it's been created\n","model_dict = model.state_dict()\n","\n","# Compare the keys of the two state dicts\n","pretrained_keys = set(model_state_dict.keys())\n","model_keys = set(model_dict.keys())\n","\n","# Find the keys that are in the model state dict but not in the pretrained state dict\n","newly_initialized_keys = model_keys - pretrained_keys\n","\n","# Print the newly initialized keys\n","print(newly_initialized_keys)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# TRAINING"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T00:24:39.076788Z","iopub.status.busy":"2024-05-14T00:24:39.076397Z","iopub.status.idle":"2024-05-14T00:24:39.089388Z","shell.execute_reply":"2024-05-14T00:24:39.088205Z","shell.execute_reply.started":"2024-05-14T00:24:39.076757Z"},"trusted":true},"outputs":[],"source":["# Loss function\n","criterion =  nn.BCEWithLogitsLoss()\n","\n","total_steps = (\n","        args.train_data_len\n","        / args.batch_sz\n","        / args.gradient_accumulation_steps\n","        * args.max_epochs)\n","\n","# Get optimizer\n","param_optimizer = list(model.named_parameters())\n","no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","optimizer_grouped_parameters = [\n","    {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n","    {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0, }]\n","optimizer = BertAdam(\n","    optimizer_grouped_parameters,\n","    lr=args.lr,\n","    warmup=args.warmup,\n","    t_total=total_steps)\n","\n","# Get scheduler\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"max\", patience=args.lr_patience, verbose=True, factor=args.lr_factor)\n","\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T00:24:40.181675Z","iopub.status.busy":"2024-05-14T00:24:40.181019Z","iopub.status.idle":"2024-05-14T00:24:40.189064Z","shell.execute_reply":"2024-05-14T00:24:40.187932Z","shell.execute_reply.started":"2024-05-14T00:24:40.181644Z"},"trusted":true},"outputs":[],"source":["for param in model.enc.img_encoder.parameters():\n","    param.requires_grad = args.freeze_img_all\n","for param in model.enc.encoder.parameters():\n","    param.requires_grad = args.freeze_txt_all"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T00:24:41.640446Z","iopub.status.busy":"2024-05-14T00:24:41.639687Z","iopub.status.idle":"2024-05-14T00:24:41.646594Z","shell.execute_reply":"2024-05-14T00:24:41.645532Z","shell.execute_reply.started":"2024-05-14T00:24:41.640412Z"},"trusted":true},"outputs":[],"source":["def model_forward(model, args, criterion, batch, device):\n","    txt, segment, mask, img, tgt = batch\n","    txt, img = txt.to(device), img.to(device)\n","    mask, segment = mask.to(device), segment.to(device)\n","    out = model(txt, mask, segment, img)\n","\n","    tgt = tgt.to(device)\n","    loss = criterion(out, tgt)\n","    return loss, out, tgt"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T00:24:44.442864Z","iopub.status.busy":"2024-05-14T00:24:44.442486Z","iopub.status.idle":"2024-05-14T00:24:44.456085Z","shell.execute_reply":"2024-05-14T00:24:44.454985Z","shell.execute_reply.started":"2024-05-14T00:24:44.442833Z"},"trusted":true},"outputs":[],"source":["def model_eval(data, model, args, criterion, device, store_preds=False):\n","    with torch.no_grad():\n","        losses, preds, preds_bool, tgts, outAUROC = [], [], [], [], []\n","        for batch in data:\n","            loss, out, tgt = model_forward(model, args, criterion, batch, device)\n","            losses.append(loss.item())\n","            \n","            pred_bool = torch.sigmoid(out).cpu().detach().numpy() > 0.5\n","            pred = torch.sigmoid(out).cpu().detach().numpy()\n","            preds.append(pred)\n","            preds_bool.append(pred_bool)\n","            tgt = tgt.cpu().detach().numpy()\n","            tgts.append(tgt)\n","\n","    metrics = {\"loss\": np.mean(losses)}\n","    classACC = dict()\n","    \n","    tgts = np.vstack(tgts)\n","    preds = np.vstack(preds)\n","    preds_bool = np.vstack(preds_bool)\n","\n","    for i in range(args.n_classes):\n","        try:\n","            outAUROC.append(roc_auc_score(tgts[:, i], preds[:, i]))\n","        except ValueError:\n","            outAUROC.append(0)\n","            pass\n","    for i in range(0, len(outAUROC)):\n","        assert args.n_classes == len(outAUROC)\n","        classACC[args.labels[i]] = outAUROC[i]\n","\n","    metrics[\"micro_roc_auc\"] = roc_auc_score(tgts, preds, average=\"micro\")\n","    metrics[\"macro_roc_auc\"] = roc_auc_score(tgts, preds, average=\"macro\")\n","    metrics[\"macro_f1\"] = f1_score(tgts, preds_bool, average=\"macro\")\n","    metrics[\"micro_f1\"] = f1_score(tgts, preds_bool, average=\"micro\")\n","    print('micro_auc:', metrics[\"micro_roc_auc\"])\n","    print('micro_f1:', metrics[\"micro_f1\"])\n","    print('-----------------------------------------------------')\n","   \n","    return metrics, classACC, tgts, preds"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T00:30:08.201969Z","iopub.status.busy":"2024-05-14T00:30:08.201572Z","iopub.status.idle":"2024-05-14T01:49:16.069280Z","shell.execute_reply":"2024-05-14T01:49:16.068060Z","shell.execute_reply.started":"2024-05-14T00:30:08.201936Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.6583281919793881\n","micro_f1: 0.2402745995423341\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.6561637158796871\n","micro_f1: 0.307277628032345\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.7299933785949649\n","micro_f1: 0.42534504391468003\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.7402004160531162\n","micro_f1: 0.4136532612369044\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.7503817502018686\n","micro_f1: 0.39297771775827145\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.7754386151405697\n","micro_f1: 0.43079096045197746\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.7882838751554967\n","micro_f1: 0.42333216905344045\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.799168126301827\n","micro_f1: 0.43415859346968066\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.8104172066507044\n","micro_f1: 0.45017421602787455\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.8198134064309432\n","micro_f1: 0.49680242342645575\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.8293224682385576\n","micro_f1: 0.5317104420243434\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.832401965543281\n","micro_f1: 0.5518321327904792\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.8404779951850517\n","micro_f1: 0.5664421310471525\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.8500954344361729\n","micro_f1: 0.5724884080370942\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.8563651318289105\n","micro_f1: 0.5943573667711599\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.8685125822889997\n","micro_f1: 0.6135729779981408\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.8749436416531805\n","micro_f1: 0.6182164392256427\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.8726919653115777\n","micro_f1: 0.6318803690741329\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.8721184368634123\n","micro_f1: 0.6393390530664125\n","-----------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344/344 [03:25<00:00,  1.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["micro_auc: 0.8721184368634123\n","micro_f1: 0.6393390530664125\n","-----------------------------------------------------\n"]}],"source":["start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n","for i_epoch in range(start_epoch, args.max_epochs):\n","    train_losses = []\n","    model.train()\n","    # model.train()\n","    optimizer.zero_grad()\n","\n","    for batch in tqdm.tqdm(train_dataloader, total=len(train_dataloader)):\n","        loss, out, target = model_forward(model, args, criterion, batch, device)\n","        if args.gradient_accumulation_steps > 1:\n","            loss = loss / args.gradient_accumulation_steps\n","\n","        train_losses.append(loss.item())\n","        loss.backward()\n","        global_step += 1\n","        if global_step % args.gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","    model.eval()\n","    metrics, classACC, tgts, preds = model_eval(val_dataloader, model, args, criterion, device)\n","\n","    tuning_metric = (\n","        metrics[\"micro_f1\"]\n","    )\n","    scheduler.step(tuning_metric)\n","    is_improvement = tuning_metric > best_metric\n","    if is_improvement:\n","        best_metric = tuning_metric\n","        n_no_improve = 0\n","        torch.save(model.state_dict(), os.path.join(args.savedir, \"best_model.pth\"))\n","    else:\n","        n_no_improve += 1\n","\n","    if n_no_improve >= args.patience:\n","        break"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T03:02:09.216722Z","iopub.status.busy":"2024-05-14T03:02:09.216344Z","iopub.status.idle":"2024-05-14T03:02:11.222804Z","shell.execute_reply":"2024-05-14T03:02:11.221875Z","shell.execute_reply.started":"2024-05-14T03:02:09.216691Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<__main__.Args object at 0x7eb246563fd0>\n"]}],"source":["model_state_dict = torch.load(\"/kaggle/working/best_model.pth\")\n","model_config = AutoConfig.from_pretrained(\"/kaggle/input/medvill-weight\")  \n","model = MultimodalBertClf.from_pretrained(\"/kaggle/input/medvill-weight\", state_dict=model_state_dict,\n","                     args=args, configs=model_config, local_files_only=True).to(device)"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T03:02:11.604425Z","iopub.status.busy":"2024-05-14T03:02:11.603952Z","iopub.status.idle":"2024-05-14T03:02:11.621120Z","shell.execute_reply":"2024-05-14T03:02:11.620097Z","shell.execute_reply.started":"2024-05-14T03:02:11.604390Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":88,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T03:05:22.214180Z","iopub.status.busy":"2024-05-14T03:05:22.213741Z","iopub.status.idle":"2024-05-14T03:05:22.583471Z","shell.execute_reply":"2024-05-14T03:05:22.580404Z","shell.execute_reply.started":"2024-05-14T03:05:22.214146Z"},"trusted":true},"outputs":[],"source":["data_iter = iter(val_dataloader)\n","data_batch = next(data_iter)\n","text_tensor, segment_tensor, mask_tensor, img_tensor, tgt_tensor =data_batch[0][2].unsqueeze(0).to(device), data_batch[1][2].unsqueeze(0).to(device), data_batch[2][2].unsqueeze(0).to(device), data_batch[3][2].unsqueeze(0).to(device), data_batch[4][2].unsqueeze(0).to(device)\n"]},{"cell_type":"code","execution_count":105,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T03:05:48.075434Z","iopub.status.busy":"2024-05-14T03:05:48.075017Z","iopub.status.idle":"2024-05-14T03:05:48.091240Z","shell.execute_reply":"2024-05-14T03:05:48.090127Z","shell.execute_reply.started":"2024-05-14T03:05:48.075402Z"},"trusted":true},"outputs":[],"source":["output = model(text_tensor, mask_tensor, segment_tensor, img_tensor)\n","pred_bool = torch.sigmoid(output).cpu().detach().numpy() > 0.5\n","pred_bool"]},{"cell_type":"code","execution_count":106,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T03:05:48.710654Z","iopub.status.busy":"2024-05-14T03:05:48.709929Z","iopub.status.idle":"2024-05-14T03:05:48.718871Z","shell.execute_reply":"2024-05-14T03:05:48.717911Z","shell.execute_reply.started":"2024-05-14T03:05:48.710618Z"},"trusted":true},"outputs":[],"source":["tgt_tensor, args.labels"]},{"cell_type":"code","execution_count":127,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T02:24:49.312378Z","iopub.status.busy":"2024-05-14T02:24:49.311952Z","iopub.status.idle":"2024-05-14T02:24:49.317224Z","shell.execute_reply":"2024-05-14T02:24:49.316077Z","shell.execute_reply.started":"2024-05-14T02:24:49.312346Z"},"trusted":true},"outputs":[],"source":["str_disease = \",\".join(diseases)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-14T02:24:51.367116Z","iopub.status.busy":"2024-05-14T02:24:51.366164Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"54e3bfe2deba4a99ae804e3f35a85465","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eb1f53791a0c48bfbd4e6110c3d82d9c","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4c11fe6fa5b34654bf4232e9dcdc1b1e","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/449 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ca8c27fed604e79ba421afb3a16dbcc","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/729 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9acf4263bb6b4240b925979f56145b8a","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"461cfd36b35f4d0c84cd1ab58793dc19","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d374d6b5bb54598bce92323dbbb3646","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00001-of-00004.bin:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"493b5fbe0fab4f81aafb6c3e8776b84e","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00002-of-00004.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa373fdc6bc64379ac665d228af4a975","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00003-of-00004.bin:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"baa88c1e8cbf4d71ab4dea5d35e6e192","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00004-of-00004.bin:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a1fdd46dffc94fc798591c97f051dd65","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Load tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(\"ruslanmv/Medical-Llama3-8B\")\n","model = AutoModelForCausalLM.from_pretrained(\"ruslanmv/Medical-Llama3-8B\").to(\"cuda\")  # If using GPU\n","# Function to format and generate response with prompt engineering using a chat template\n","def askme(question):\n","    sys_message = ''' \n","    You are an AI Medical Assistant trained on a vast dataset of health information. Please be thorough and\n","    provide an informative answer. If you don't know the answer to a specific medical inquiry, advise seeking professional help.\n","    '''\n","    \n","    # Create messages structured for the chat template\n","    messages = [{\"role\": \"system\", \"content\": sys_message}, {\"role\": \"user\", \"content\": question}]\n","\n","    # Applying chat template\n","    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","    outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)  # Adjust max_new_tokens for longer responses\n","\n","    # Extract and return the generated text\n","    answer = tokenizer.batch_decode(outputs)[0].strip()\n","    return answer\n","\n","# Example usage\n","# - Context: First describe your problem.\n","# - Question: Then make the question.\n","question = f\"I am suffering from the following diseases: {diseases}, please suggest me habits I need to change as well as methods to treat these diseases.\"\n","print(askme(question))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to format and generate response with prompt engineering using a chat template\n","def askme(question):\n","    sys_message = ''' \n","    You are an AI Medical Assistant trained on a vast dataset of health information. Please be thorough and\n","    provide an informative answer. If you don't know the answer to a specific medical inquiry, advise seeking professional help.\n","    '''\n","    \n","    # Create messages structured for the chat template\n","    messages = [{\"role\": \"system\", \"content\": sys_message}, {\"role\": \"user\", \"content\": question}]\n","\n","    # Applying chat template\n","    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","    outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)  # Adjust max_new_tokens for longer responses\n","\n","    # Extract and return the generated text\n","    answer = tokenizer.batch_decode(outputs)[0].strip()\n","    return answer\n","\n","# Example usage\n","# - Context: First describe your problem.\n","# - Question: Then make the question.\n","question = f\"I am suffering from the following diseases: {diseases}, please suggest me habits I need to change as well as methods to treat this disease.\"\n","print(askme(question))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4997626,"sourceId":8399810,"sourceType":"datasetVersion"},{"datasetId":4993224,"sourceId":8399872,"sourceType":"datasetVersion"},{"datasetId":4946895,"sourceId":8404239,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
