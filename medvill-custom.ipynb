{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"collapsed_sections":["wuGwvW0mUstb","1j81M9GQPJoQ","JcYMmFuJUDyi","M-NKJuanUG0R","YqFuvx_DWdAI","L9zj_upyXoIn"]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10801109,"sourceType":"datasetVersion","datasetId":6703776},{"sourceId":10806635,"sourceType":"datasetVersion","datasetId":6649102}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **0. UTILS**","metadata":{"id":"wuGwvW0mUstb"}},{"cell_type":"markdown","source":"## 0.1 Utils","metadata":{}},{"cell_type":"code","source":"import contextlib\nimport numpy as np\nimport random\nimport shutil\nimport os\n\nimport torch","metadata":{"trusted":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2025-02-20T16:48:25.018809Z","iopub.execute_input":"2025-02-20T16:48:25.019015Z","iopub.status.idle":"2025-02-20T16:48:29.070957Z","shell.execute_reply.started":"2025-02-20T16:48:25.018994Z","shell.execute_reply":"2025-02-20T16:48:29.070009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:48:29.072477Z","iopub.execute_input":"2025-02-20T16:48:29.072867Z","iopub.status.idle":"2025-02-20T16:48:29.077458Z","shell.execute_reply.started":"2025-02-20T16:48:29.072840Z","shell.execute_reply":"2025-02-20T16:48:29.076218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def truncate_seq_pair(tokens_a, tokens_b, max_length):\n    \"\"\"Truncates a sequence pair in place to the maximum length.\n    Copied from https://github.com/huggingface/pytorch-pretrained-BERT\n    \"\"\"\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()","metadata":{"id":"FGYrtHSaU7OE","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:48:29.081680Z","iopub.execute_input":"2025-02-20T16:48:29.081965Z","iopub.status.idle":"2025-02-20T16:48:29.097704Z","shell.execute_reply.started":"2025-02-20T16:48:29.081945Z","shell.execute_reply":"2025-02-20T16:48:29.096840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@contextlib.contextmanager\ndef numpy_seed(seed, *addl_seeds):\n    \"\"\"Context manager which seeds the NumPy PRNG with the specified seed and\n    restores the state afterward\"\"\"\n    if seed is None:\n        yield\n        return\n    if len(addl_seeds) > 0:\n        seed = int(hash((seed, *addl_seeds)) % 1e6)\n    state = np.random.get_state()\n    np.random.seed(seed)\n    try:\n        yield\n    finally:\n        np.random.set_state(state)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:48:29.098518Z","iopub.execute_input":"2025-02-20T16:48:29.098831Z","iopub.status.idle":"2025-02-20T16:48:29.114731Z","shell.execute_reply.started":"2025-02-20T16:48:29.098798Z","shell.execute_reply":"2025-02-20T16:48:29.113930Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 0.2 Initialize arguments","metadata":{}},{"cell_type":"markdown","source":"> Khởi tạo đối số cố định thay vì nhập","metadata":{}},{"cell_type":"code","source":"class Args:\n    def __init__(self):\n        self.seed = 123\n        self.batch_sz = 8\n        self.max_epochs = 20\n        self.task_type = \"multilabel\"\n        self.n_workers = 4\n        self.patience = 20\n        \n        output_path = 'output'\n        self.savedir = \"/kaggle/working\"\n        self.save_name = 'mimic_par'\n        \n        self.loaddir = '/kaggle/input/medvill-weight'\n        self.name = \"scenario_name\"\n        \n        self.openi = False\n        self.data_path = '/kaggle/input/thesis-mimic-cxr-2-0-0'\n        self.Train_dset_name = 'train.jsonl'\n        self.Valid_dset_name = 'valid.jsonl'\n\n        self.embed_sz = 768\n        self.hidden_sz = 768\n        self.bert_model = \"bert-base-uncased\"\n        self.init_model = \"bert-base-uncased\"\n        \n        self.drop_img_percent = 0.0\n        self.dropout = 0.1\n        \n        self.freeze_img = 0\n        self.freeze_txt = 0\n        \n        self.freeze_img_all = False\n        self.freeze_txt_all = False\n        \n        self.glove_path = \"/path/to/glove_embeds/glove.840B.300d.txt\"\n        self.gradient_accumulation_steps = 2\n        self.hidden = []\n        \n        self.img_embed_pool_type = \"avg\"\n        self.img_hidden_sz = 2048\n        self.include_bn = True\n        \n        self.lr = 1e-3\n        self.lr_factor = 0.75\n        self.lr_patience = 5\n        \n        self.max_seq_len = 512\n        self.num_image_embeds = 256\n        \n        self.warmup = 0.1\n        self.weight_classes = 1\n    \nargs = Args()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:48:29.116467Z","iopub.execute_input":"2025-02-20T16:48:29.116722Z","iopub.status.idle":"2025-02-20T16:48:29.133168Z","shell.execute_reply.started":"2025-02-20T16:48:29.116703Z","shell.execute_reply":"2025-02-20T16:48:29.132218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **1. DATA**","metadata":{"id":"1j81M9GQPJoQ"}},{"cell_type":"markdown","source":"## 1.1 Vocab","metadata":{"id":"JcYMmFuJUDyi"}},{"cell_type":"code","source":"\nclass Vocab(object):\n    \"\"\"\n    A vocabulary class that maps words to indices and vice versa.\n\n    - If `emptyInit=True`, initializes an empty vocabulary.\n    - If `emptyInit=False`, initializes with special tokens: [PAD], [UNK], [CLS], [SEP], [MASK].\n\n    Attributes:\n        stoi (dict): Maps words to indices.\n        itos (list): Maps indices to words.\n        vocab_sz (int): Size of the vocabulary.\n\n    Methods:\n        add(words): Adds new words to the vocabulary if they don't exist.\n    \"\"\"\n    def __init__(self, emptyInit=False):\n        if emptyInit:\n            self.stoi, self.itos, self.vocab_sz = {}, [], 0\n        else:\n            self.stoi = {\n                w: i\n                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n            }\n            self.itos = [w for w in self.stoi]\n            self.vocab_sz = len(self.itos)\n\n    def add(self, words):\n        cnt = len(self.itos)\n        for w in words:\n            if w in self.stoi:\n                continue\n            self.stoi[w] = cnt\n            self.itos.append(w)\n            cnt += 1\n        self.vocab_sz = len(self.itos)","metadata":{"id":"fW0euaVhPE64","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:48:29.134483Z","iopub.execute_input":"2025-02-20T16:48:29.134781Z","iopub.status.idle":"2025-02-20T16:48:29.154356Z","shell.execute_reply.started":"2025-02-20T16:48:29.134761Z","shell.execute_reply":"2025-02-20T16:48:29.153558Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.2 Dataset","metadata":{"id":"M-NKJuanUG0R"}},{"cell_type":"markdown","source":"> Set ImageFile.LOAD_TRUNCATED_IMAGES = True để bỏ qua những ảnh hư","metadata":{}},{"cell_type":"code","source":"import json\nimport numpy as np\nimport os\nfrom PIL import Image\n\nimport torch\nfrom torch.utils.data import Dataset\n\n# from utils.utils import truncate_seq_pair, numpy_seed\n\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True","metadata":{"id":"IBHtevk7UIJb","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:48:29.155311Z","iopub.execute_input":"2025-02-20T16:48:29.155706Z","iopub.status.idle":"2025-02-20T16:48:29.174460Z","shell.execute_reply.started":"2025-02-20T16:48:29.155671Z","shell.execute_reply":"2025-02-20T16:48:29.173759Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Convert RGB để 3 chiều","metadata":{}},{"cell_type":"code","source":"class JsonlDataset(Dataset):\n    \"\"\"Dataset for processing .jsonl files containing text and image data.\n\n    - Tokenizes text, truncates it to max length, and converts tokens to indices.\n    - Handles optional image data, applying transformations if available.\n    - Supports multilabel classification by converting labels into tensors.\n\n    Attributes:\n        data (list): List of parsed JSON objects from the dataset file.\n        tokenizer (callable): Tokenizer function to process text.\n        vocab (Vocab): Vocabulary object for token-to-index conversion.\n        transforms (callable): Transformations applied to images.\n        n_classes (int): Number of output classes.\n        max_seq_len (int): Maximum sequence length after accounting for images.\n\n    Methods:\n        __len__(): Returns the number of samples.\n        __getitem__(index): Retrieves a sample, processing text, images, and labels.\n    \"\"\"\n    def __init__(self, data_path, tokenizer, transforms, vocab, args):\n        self.data = [json.loads(l) for l in open(data_path)]\n        self.data_dir = os.path.dirname(data_path)\n        self.tokenizer = tokenizer\n        self.args = args\n        self.vocab = vocab\n        self.n_classes = len(args.labels)\n        self.text_start_token =  [\"[SEP]\"]\n\n        # Drop images randomly for generalization\n        with numpy_seed(0):\n            for row in self.data:\n                if np.random.random() < args.drop_img_percent:\n                    row[\"img\"] = None\n\n        self.max_seq_len = args.max_seq_len\n        self.max_seq_len -= args.num_image_embeds\n\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        sentence = (\n            self.text_start_token\n            + self.tokenizer(self.data[index][\"text\"])[: (self.max_seq_len - 1)] \n            + self.text_start_token\n        )\n        segment = torch.zeros(len(sentence))\n        sentence = torch.LongTensor(\n            [\n                self.vocab.stoi[w] if w in self.vocab.stoi else self.vocab.stoi[\"[UNK]\"]\n                for w in sentence\n            ]\n        )\n        if self.args.task_type == \"multilabel\":\n            label = torch.zeros(self.n_classes)\n            if self.data[index][\"label\"] == '':\n                self.data[index][\"label\"] = \"'Others'\"\n            else:\n                pass\n            label[\n                [self.args.labels.index(tgt) for tgt in self.data[index][\"label\"]]\n            ] = 1\n        else:\n            pass\n\n        image = None\n        if self.data[index][\"img\"]:\n            image = Image.open(\n                os.path.join(self.data_dir, self.data[index][\"img\"])).convert(\"RGB\")\n        else:\n            image = Image.fromarray(128 * np.ones((512, 512, 3), dtype=np.uint8))\n        image = self.transforms(image)\n\n        # The first SEP is part of Image Token.\n        segment = segment[1:]\n        sentence = sentence[1:]\n        # The first segment (0) is of images.\n        segment += 1\n\n        return sentence, segment, image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:48:39.295859Z","iopub.execute_input":"2025-02-20T16:48:39.296326Z","iopub.status.idle":"2025-02-20T16:48:39.312593Z","shell.execute_reply.started":"2025-02-20T16:48:39.296283Z","shell.execute_reply":"2025-02-20T16:48:39.310973Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.3 Helpers","metadata":{"id":"YqFuvx_DWdAI"}},{"cell_type":"code","source":"!pip install pytorch-pretrained-bert","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:48:48.463353Z","iopub.execute_input":"2025-02-20T16:48:48.463666Z","iopub.status.idle":"2025-02-20T16:48:53.508828Z","shell.execute_reply.started":"2025-02-20T16:48:48.463640Z","shell.execute_reply":"2025-02-20T16:48:53.507692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import functools\nimport json\nimport os\nfrom collections import Counter\n\nimport torch\nimport torchvision.transforms as transforms\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom torch.utils.data import DataLoader\n\n# from data.dataset import JsonlDataset\n# from data.vocab import Vocab","metadata":{"id":"OverYFy6Wf1F","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:48:53.510352Z","iopub.execute_input":"2025-02-20T16:48:53.510739Z","iopub.status.idle":"2025-02-20T16:48:56.951982Z","shell.execute_reply.started":"2025-02-20T16:48:53.510704Z","shell.execute_reply":"2025-02-20T16:48:56.950906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_transforms(args):\n    \"\"\"Returns a set of image transformations based on the dataset source.\n\n    - If args.openi is True, converts images to grayscale with 3 channels.\n    - Converts images to tensors and normalizes using ImageNet statistics.\n\n    Args:\n        args: An object containing configuration settings.\n\n    Returns:\n        torchvision.transforms.Compose: A sequence of image transformations.\n    \"\"\"\n    # if args.openi:\n    #     return transforms.Compose(\n    #         [\n    #             transforms.Grayscale(num_output_channels=3),\n    #             transforms.ToTensor(),\n    #             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    #         ])\n    # else:\n    return transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:48:56.953428Z","iopub.execute_input":"2025-02-20T16:48:56.953934Z","iopub.status.idle":"2025-02-20T16:48:56.959713Z","shell.execute_reply.started":"2025-02-20T16:48:56.953897Z","shell.execute_reply":"2025-02-20T16:48:56.958698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_labels_and_frequencies(path):\n    \"\"\"Extracts unique labels and their frequencies from a JSONL file.\n\n    - Reads labels from the dataset.\n    - Splits multi-label entries and replaces empty labels with 'Others'.\n    - Counts occurrences of each label.\n\n    Args:\n        path (str): Path to the JSONL file.\n\n    Returns:\n        tuple: (List of unique labels, Counter object with label frequencies).\n    \"\"\"\n    label_freqs = Counter()\n    data_labels = [json.loads(line)[\"label\"] for line in open(path)]\n    if type(data_labels) == list:\n        # Change a little here to read our csv\n        for label_row in data_labels:\n            label_freqs.update(label_row)\n    else:\n        pass\n    return list(label_freqs.keys()), label_freqs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:48:58.674035Z","iopub.execute_input":"2025-02-20T16:48:58.674383Z","iopub.status.idle":"2025-02-20T16:48:58.679303Z","shell.execute_reply.started":"2025-02-20T16:48:58.674338Z","shell.execute_reply":"2025-02-20T16:48:58.678197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_vocab(args):\n    vocab = Vocab()\n    bert_tokenizer = BertTokenizer.from_pretrained(\n        args.bert_model, do_lower_case=True\n    )\n    vocab.stoi = bert_tokenizer.vocab\n    vocab.itos = bert_tokenizer.ids_to_tokens\n    vocab.vocab_sz = len(vocab.itos)\n\n    return vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:49:02.373751Z","iopub.execute_input":"2025-02-20T16:49:02.374037Z","iopub.status.idle":"2025-02-20T16:49:02.378260Z","shell.execute_reply.started":"2025-02-20T16:49:02.374015Z","shell.execute_reply":"2025-02-20T16:49:02.377354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch, args):\n    lens = [len(row[0]) for row in batch]\n    bsz, max_seq_len = len(batch), max(lens)\n\n    mask_tensor = torch.zeros(bsz, max_seq_len).long()\n    text_tensor = torch.zeros(bsz, max_seq_len).long()\n    segment_tensor = torch.zeros(bsz, max_seq_len).long()\n\n    img_tensor = None\n    img_tensor = torch.stack([row[2] for row in batch])\n\n    if args.task_type == \"multilabel\":\n        # Multilabel case\n        tgt_tensor = torch.stack([row[3] for row in batch])\n    else:\n        # Single Label case\n        tgt_tensor = torch.cat([row[3] for row in batch]).long()\n\n    for i_batch, (input_row, length) in enumerate(zip(batch, lens)):\n        tokens, segment = input_row[:2]\n        text_tensor[i_batch, :length] = tokens\n        segment_tensor[i_batch, :length] = segment\n        mask_tensor[i_batch, :length] = 1\n\n    return text_tensor, segment_tensor, mask_tensor, img_tensor, tgt_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:49:06.312574Z","iopub.execute_input":"2025-02-20T16:49:06.312913Z","iopub.status.idle":"2025-02-20T16:49:06.319664Z","shell.execute_reply.started":"2025-02-20T16:49:06.312884Z","shell.execute_reply":"2025-02-20T16:49:06.318652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_data_loaders(args):\n    tokenizer = (\n        BertTokenizer.from_pretrained(args.bert_model, do_lower_case=True).tokenize)\n\n    transforms = get_transforms(args)\n\n    args.labels, args.label_freqs = get_labels_and_frequencies(\n        os.path.join(args.data_path, args.Train_dset_name)\n    )\n\n    vocab = get_vocab(args)\n    args.vocab = vocab\n    args.vocab_sz = vocab.vocab_sz\n    args.n_classes = len(args.labels)\n\n    train = JsonlDataset(\n        os.path.join(args.data_path, args.Train_dset_name),\n        tokenizer,\n        transforms,\n        vocab,\n        args,\n    )\n\n    args.train_data_len = len(train)\n\n    dev = JsonlDataset(\n        os.path.join(args.data_path, args.Valid_dset_name),\n        tokenizer,\n        transforms,\n        vocab,\n        args,\n    )\n\n    collate = functools.partial(collate_fn, args=args)\n\n    train_loader = DataLoader(\n        train,\n        batch_size=args.batch_sz,\n        shuffle=True,\n        num_workers=args.n_workers,\n        collate_fn=collate,\n    )\n\n    val_loader = DataLoader(\n        dev,\n        batch_size=args.batch_sz,\n        shuffle=False,\n        num_workers=args.n_workers,\n        collate_fn=collate,\n    )\n\n    return train_loader, val_loader  # , test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:49:11.060014Z","iopub.execute_input":"2025-02-20T16:49:11.060335Z","iopub.status.idle":"2025-02-20T16:49:11.066389Z","shell.execute_reply.started":"2025-02-20T16:49:11.060309Z","shell.execute_reply":"2025-02-20T16:49:11.065597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_loader, val_loader = get_data_loaders(args)\n# next(iter(train_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T08:42:39.389024Z","iopub.execute_input":"2025-02-20T08:42:39.389321Z","iopub.status.idle":"2025-02-20T08:42:39.392901Z","shell.execute_reply.started":"2025-02-20T08:42:39.389298Z","shell.execute_reply":"2025-02-20T08:42:39.392098Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **2. MODELS**","metadata":{"id":"xs8qKTFTXaED"}},{"cell_type":"markdown","source":"## 2.1 Image","metadata":{"id":"L9zj_upyXoIn"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision.transforms import ToTensor\nfrom PIL import Image\nimport torch.nn.functional as F\nfrom einops import rearrange\nfrom glob import glob","metadata":{"id":"JuOe7cvqXdkC","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:49:14.603966Z","iopub.execute_input":"2025-02-20T16:49:14.604327Z","iopub.status.idle":"2025-02-20T16:49:14.630553Z","shell.execute_reply.started":"2025-02-20T16:49:14.604293Z","shell.execute_reply":"2025-02-20T16:49:14.629868Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Lấy đặc trưng của ảnh qua mô hình ResNet50.\n- Flattening các đặc trưng để chuẩn bị cho các bước xử lý sau.","metadata":{"id":"GM0e1bGFWyU0"}},{"cell_type":"code","source":"class ImageEncoder(nn.Module):\n    def __init__(self, args):\n        super(ImageEncoder, self).__init__()\n        self.args = args\n        model = torchvision.models.resnet50(pretrained=True)\n        modules = list(model.children())[:-2]\n        self.model = nn.Sequential(*modules)\n    '''\n        pool_func = (\n            nn.AdaptiveAvgPool2d\n            if args.img_embed_pool_type == \"avg\"\n            else nn.AdaptiveMaxPool2d\n        )\n\n        if args.num_image_embeds in [1, 2, 3, 5, 7]:\n            self.pool = pool_func((args.num_image_embeds, 1))\n        elif args.num_image_embeds == 4:\n            self.pool = pool_func((2, 2))\n        elif args.num_image_embeds == 6:\n            self.pool = pool_func((3, 2))\n        elif args.num_image_embeds == 8:\n            self.pool = pool_func((4, 2))\n        elif args.num_image_embeds == 9:\n            self.pool = pool_func((3, 3))\n    '''\n    def forward(self, x):\n        # Bx3x224x224 -> Bx2048x7x7 -> Bx2048xN -> BxNx2048\n\n        # out = self.pool(self.model(x))\n        # out = torch.flatten(out, start_dim=2)\n        # out = out.transpose(1, 2).contiguous()\n\n        out = self.model(x)\n        out = torch.flatten(out, start_dim=2) #out torch.Size([100, 2048, 3])\n        out = out.transpose(1, 2).contiguous() #out torch.Size([100, 3, 2048])\n\n        # print(\"out.size()\",out.size())\n        # input(\"STOP!!!\")\n\n\n        return out  # BxNx2048","metadata":{"id":"B2leO2vPAVD0","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:49:23.739086Z","iopub.execute_input":"2025-02-20T16:49:23.739433Z","iopub.status.idle":"2025-02-20T16:49:23.745014Z","shell.execute_reply.started":"2025-02-20T16:49:23.739407Z","shell.execute_reply":"2025-02-20T16:49:23.744195Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.2 Model","metadata":{"id":"TRZDg1_oXzq1"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertConfig, BertModel\n# from models.image import ImageEncoder","metadata":{"id":"K2FSjYHQZKGB","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:49:26.959838Z","iopub.execute_input":"2025-02-20T16:49:26.960123Z","iopub.status.idle":"2025-02-20T16:49:43.133461Z","shell.execute_reply.started":"2025-02-20T16:49:26.960100Z","shell.execute_reply":"2025-02-20T16:49:43.132464Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Trước tiên, lớp chuyển đổi hình ảnh thành embeddings.\n- Tiếp theo, mã hóa các token `[CLS]` và `[SEP]` để bao gồm chúng vào chuỗi embeddings.\n- Cuối cùng, các embeddings của văn bản và hình ảnh được cộng với embeddings vị trí và loại token, sau đó chuẩn hóa và thực hiện dropout.","metadata":{"id":"yddVGTjG3KnX"}},{"cell_type":"code","source":"class ImageBertEmbeddings(nn.Module):\n    def __init__(self, args, embeddings):\n        super(ImageBertEmbeddings, self).__init__()\n        self.args = args\n        self.img_embeddings = nn.Linear(args.img_hidden_sz, args.hidden_sz)\n        self.position_embeddings = embeddings.position_embeddings\n        self.token_type_embeddings = embeddings.token_type_embeddings\n        self.word_embeddings = embeddings.word_embeddings\n        self.LayerNorm = embeddings.LayerNorm\n        self.dropout = nn.Dropout(p=args.dropout)\n\n    def forward(self, input_imgs, token_type_ids):\n        bsz = input_imgs.size(0)\n        seq_length = self.args.num_image_embeds + 2  # +2 for CLS and SEP Token\n\n        # CLS Token\n        cls_id = torch.LongTensor([self.args.vocab.stoi[\"[CLS]\"]]).cuda()\n        cls_id = cls_id.unsqueeze(0).expand(bsz, 1)\n        cls_token_embeds = self.word_embeddings(cls_id)\n\n        # SEP Token\n        sep_id = torch.LongTensor([self.args.vocab.stoi[\"[SEP]\"]]).cuda()\n        sep_id = sep_id.unsqueeze(0).expand(bsz, 1)\n        sep_token_embeds = self.word_embeddings(sep_id)\n\n        # Image embeddings\n        imgs_embeddings = self.img_embeddings(input_imgs)\n        token_embeddings = torch.cat(\n            [cls_token_embeds, imgs_embeddings, sep_token_embeds], dim=1)\n\n        # Position and token type embeddings\n        position_ids = torch.arange(seq_length, dtype=torch.long).cuda()\n        position_ids = position_ids.unsqueeze(0).expand(bsz, seq_length)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        # Final embeddings\n        # print('token_sz:', token_embeddings.size())\n        # print('position_sz:', position_embeddings.size())\n        # print('token_type:', token_type_embeddings.size())\n        embeddings = token_embeddings + position_embeddings + token_type_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n\n        return embeddings","metadata":{"id":"ANahuAfBxXEl","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:49:43.134743Z","iopub.execute_input":"2025-02-20T16:49:43.135287Z","iopub.status.idle":"2025-02-20T16:49:43.142948Z","shell.execute_reply.started":"2025-02-20T16:49:43.135264Z","shell.execute_reply":"2025-02-20T16:49:43.141949Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Tạo attention mask để xử lý cả văn bản và hình ảnh.\n- Trích xuất đặc trưng hình ảnh bằng ResNet50 (ImageEncoder).\n- Tạo embeddings cho hình ảnh và văn bản.\n- Ghép embeddings hình ảnh và văn bản thành một chuỗi đầu vào cho BERT.\n- Truyền dữ liệu qua BERT encoder để học các mối quan hệ giữa hình ảnh và văn bản.\n- Trả về embedding cuối cùng để sử dụng cho các tác vụ tiếp theo.","metadata":{"id":"Nd-r2YYCGiF8"}},{"cell_type":"markdown","source":"> Sửa lại float 16 thành float 32 trong hàm forward","metadata":{}},{"cell_type":"code","source":"class MultimodalBertEncoder(nn.Module):\n    def __init__(self, args):\n        super(MultimodalBertEncoder, self).__init__()\n        self.args = args\n\n        if args.init_model == \"bert-base-scratch\":\n            config = BertConfig.from_pretrained(\"bert-base-uncased\")\n            bert = BertModel(config)\n        else:\n            bert = BertModel.from_pretrained(args.init_model)\n        self.txt_embeddings = bert.embeddings\n        \n        self.img_embeddings = ImageBertEmbeddings(args, self.txt_embeddings)\n        self.img_encoder = ImageEncoder(args)\n        self.encoder = bert.encoder\n        self.pooler = bert.pooler\n        #self.clf = nn.Linear(args.hidden_sz, args.n_classes)\n\n    def forward(self, input_txt, attention_mask, segment, input_img):\n        bsz = input_txt.size(0)\n        \n        # Attention mask for both text and image\n        attention_mask = torch.cat(\n            [\n                torch.ones(bsz, self.args.num_image_embeds + 2).long().cuda(),\n                attention_mask,\n            ],\n            dim=1)\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        try:\n            extended_attention_mask = extended_attention_mask.to(\n                dtype=next(self.parameters()).dtype)  # fp16 compatibility\n        except StopIteration:\n            extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # Image embedding\n        img_tok = (\n            torch.LongTensor(input_txt.size(0), self.args.num_image_embeds + 2)\n            .fill_(0)\n            .cuda())\n        img = self.img_encoder(input_img)  # BxNx3x224x224 -> BxNx2048\n\n        img_embed_out = self.img_embeddings(img, img_tok)\n        txt_embed_out = self.txt_embeddings(input_txt, segment)\n        encoder_input = torch.cat([img_embed_out, txt_embed_out], 1)  # Bx(TEXT+IMG)xHID\n        encoded_layers = self.encoder(encoder_input, extended_attention_mask)\n        \n        return self.pooler(encoded_layers[-1])","metadata":{"id":"udLQu76rxU-L","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:50:36.194762Z","iopub.execute_input":"2025-02-20T16:50:36.195112Z","iopub.status.idle":"2025-02-20T16:50:36.202661Z","shell.execute_reply.started":"2025-02-20T16:50:36.195087Z","shell.execute_reply":"2025-02-20T16:50:36.201687Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Lấy đầu ra từ MultimodalBertEncoder.\n- Đưa đầu ra qua lớp phân loại (clf).","metadata":{"id":"C3_UXbBsG3OI"}},{"cell_type":"code","source":"class MultimodalBertClf(nn.Module):\n    def __init__(self, args):\n        super(MultimodalBertClf, self).__init__()\n        self.args = args\n        self.enc = MultimodalBertEncoder(args)\n        self.clf = nn.Linear(args.hidden_sz, args.n_classes)\n\n    def forward(self, txt, mask, segment, img):\n        x = self.enc(txt, mask, segment, img)\n        return self.clf(x)","metadata":{"id":"LyruNDrKxTH6","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:50:40.614218Z","iopub.execute_input":"2025-02-20T16:50:40.614575Z","iopub.status.idle":"2025-02-20T16:50:40.619556Z","shell.execute_reply.started":"2025-02-20T16:50:40.614549Z","shell.execute_reply":"2025-02-20T16:50:40.618542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODELS = {\n    \"model\": MultimodalBertClf,\n}\n\ndef get_model(args):\n    return MODELS['model'](args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:50:44.291766Z","iopub.execute_input":"2025-02-20T16:50:44.292076Z","iopub.status.idle":"2025-02-20T16:50:44.296171Z","shell.execute_reply.started":"2025-02-20T16:50:44.292042Z","shell.execute_reply":"2025-02-20T16:50:44.295044Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **3. Main**","metadata":{"id":"JWCONO7pcHrZ"}},{"cell_type":"code","source":"import os\nimport csv\nimport argparse\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom datetime import datetime\nimport torch.optim as optim\nfrom pytorch_pretrained_bert import BertAdam\nfrom sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n# from data.helpers import get_data_loaders\n# from utils.utils import *","metadata":{"id":"ppanGIy1cK9h","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:50:48.580085Z","iopub.execute_input":"2025-02-20T16:50:48.580449Z","iopub.status.idle":"2025-02-20T16:50:48.584714Z","shell.execute_reply.started":"2025-02-20T16:50:48.580421Z","shell.execute_reply":"2025-02-20T16:50:48.583955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_criterion(args, device):\n    if args.task_type == \"multilabel\":\n        if args.weight_classes:\n            freqs = [args.label_freqs[l] for l in args.labels]\n            negative = [args.train_data_len - l for l in freqs]\n            label_weights = (torch.FloatTensor(freqs) / torch.FloatTensor(negative)) ** -1\n            criterion = nn.BCEWithLogitsLoss(pos_weight=label_weights.to(device))\n        else:\n            criterion = nn.BCEWithLogitsLoss()\n    else:\n        criterion = nn.CrossEntropyLoss()\n    return criterion","metadata":{"id":"1Us8IVA5HgPE","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:50:51.020047Z","iopub.execute_input":"2025-02-20T16:50:51.020408Z","iopub.status.idle":"2025-02-20T16:50:51.025301Z","shell.execute_reply.started":"2025-02-20T16:50:51.020378Z","shell.execute_reply":"2025-02-20T16:50:51.024378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_optimizer(model, args):\n    total_steps = (\n            args.train_data_len\n            / args.batch_sz\n            / args.gradient_accumulation_steps\n            * args.max_epochs)\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0, }]\n    optimizer = BertAdam(\n        optimizer_grouped_parameters,\n        lr=args.lr,\n        warmup=args.warmup,\n        t_total=total_steps)\n    return optimizer","metadata":{"id":"Chj-b0kxHeau","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:50:54.239414Z","iopub.execute_input":"2025-02-20T16:50:54.239704Z","iopub.status.idle":"2025-02-20T16:50:54.245370Z","shell.execute_reply.started":"2025-02-20T16:50:54.239681Z","shell.execute_reply":"2025-02-20T16:50:54.244612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_scheduler(optimizer, args):\n    return optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \"max\", patience=args.lr_patience, verbose=True, factor=args.lr_factor\n    )","metadata":{"id":"9PZ_YJ1THcrO","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:50:56.470617Z","iopub.execute_input":"2025-02-20T16:50:56.470895Z","iopub.status.idle":"2025-02-20T16:50:56.474775Z","shell.execute_reply.started":"2025-02-20T16:50:56.470873Z","shell.execute_reply":"2025-02-20T16:50:56.474033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def model_eval(data, model, args, criterion, device, store_preds=False):\n    with torch.no_grad():\n        losses, preds, preds_bool, tgts, outAUROC = [], [], [], [], []\n        for batch in data:\n            loss, out, tgt = model_forward(model, args, criterion, batch, device)\n            losses.append(loss.item())\n            if args.task_type == \"multilabel\":\n                pred_bool = torch.sigmoid(out).cpu().detach().numpy() > 0.5\n                pred = torch.sigmoid(out).cpu().detach().numpy()\n            else:pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n            preds.append(pred)\n            preds_bool.append(pred_bool)\n            tgt = tgt.cpu().detach().numpy()\n            tgts.append(tgt)\n\n    metrics = {\"loss\": np.mean(losses)}\n    classACC = dict()\n    if args.task_type == \"multilabel\":\n        tgts = np.vstack(tgts)\n        preds = np.vstack(preds)\n        preds_bool = np.vstack(preds_bool)\n\n        for i in range(args.n_classes):\n            try:\n                outAUROC.append(roc_auc_score(tgts[:, i], preds[:, i]))\n            except ValueError:\n                outAUROC.append(0)\n                pass\n        for i in range(0, len(outAUROC)):\n            assert args.n_classes == len(outAUROC)\n            classACC[args.labels[i]] = outAUROC[i]\n\n        metrics[\"micro_roc_auc\"] = roc_auc_score(tgts, preds, average=\"micro\")\n        metrics[\"macro_roc_auc\"] = roc_auc_score(tgts, preds, average=\"macro\")\n        metrics[\"macro_f1\"] = f1_score(tgts, preds_bool, average=\"macro\")\n        metrics[\"micro_f1\"] = f1_score(tgts, preds_bool, average=\"micro\")\n        print('micro_auc:', metrics[\"micro_roc_auc\"])\n        print('micro_f1:', metrics[\"micro_f1\"])\n        print('-----------------------------------------------------')\n    else:\n        tgts = [l for sl in tgts for l in sl]\n        preds = [l for sl in preds for l in sl]\n        metrics[\"acc\"] = accuracy_score(tgts, preds)\n\n    if store_preds:\n        store_preds_to_disk(tgts, preds, args)\n\n    return metrics, classACC, tgts, preds","metadata":{"id":"onQVcO3sHaRy","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:50:57.057464Z","iopub.execute_input":"2025-02-20T16:50:57.057767Z","iopub.status.idle":"2025-02-20T16:50:57.067643Z","shell.execute_reply.started":"2025-02-20T16:50:57.057744Z","shell.execute_reply":"2025-02-20T16:50:57.066765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def model_forward(model, args, criterion, batch, device):\n    txt, segment, mask, img, tgt = batch\n    model.to(device)\n    if args.num_image_embeds > 0:\n        for param in model.module.enc.img_encoder.parameters():\n            param.requires_grad = args.freeze_img_all\n    for param in model.module.enc.encoder.parameters():\n        param.requires_grad = args.freeze_txt_all\n\n    txt, img = txt.to(device), img.to(device)\n    mask, segment = mask.to(device), segment.to(device)\n    out = model(txt, mask, segment, img)\n\n    tgt = tgt.to(device)\n    loss = criterion(out, tgt)\n    return loss, out, tgt","metadata":{"id":"0_gZlPl7HWqj","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:51:00.106843Z","iopub.execute_input":"2025-02-20T16:51:00.107211Z","iopub.status.idle":"2025-02-20T16:51:00.112755Z","shell.execute_reply.started":"2025-02-20T16:51:00.107176Z","shell.execute_reply":"2025-02-20T16:51:00.111678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(args):\n    print(\"Training start!!\")\n    print(\" # PID :\", os.getpid())\n    \n    set_seed(args.seed)\n    args.savedir = os.path.join(args.savedir, args.save_name)\n    os.makedirs(args.savedir, exist_ok=True)\n    \n    train_loader, val_loader = get_data_loaders(args)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = get_model(args)\n    \n    criterion = get_criterion(args, device)\n    optimizer = get_optimizer(model, args)\n    scheduler = get_scheduler(optimizer, args)\n    \n    # logger = create_logger(\"%s/logfile.log\" % args.savedir, args)\n    torch.save(args, os.path.join(args.savedir, \"args.bin\"))\n    \n    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n    \n    if os.path.exists(os.path.join(args.loaddir, \"pytorch_model.bin\")):\n        model.load_state_dict(torch.load(args.loaddir + \"/pytorch_model.bin\"), strict=False)\n    \n        print(\"This would load the trained model, then fine-tune the model.\")\n    \n    else:\n        print(\"\")\n        print(\"\")\n        print(\"this option initilize the model with random value. train from scratch.\")\n        print(\"Loaded model : \")\n    \n    \n    \n    print(\"freeze image?\", args.freeze_img_all)\n    print(\"freeze txt?\", args.freeze_txt_all)\n    model.to(device)\n    # logger.info(\"Training..\")\n    \n    if torch.cuda.device_count() > 1:\n        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n        model = nn.DataParallel(model)\n    \n    for i_epoch in range(start_epoch, args.max_epochs):\n        train_losses = []\n        # model.module.train()\n        model.train()\n        optimizer.zero_grad()\n    \n        for batch in tqdm(train_loader, total=len(train_loader)):\n            loss, out, target = model_forward(model, args, criterion, batch, device)\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n    \n            train_losses.append(loss.item())\n            loss.backward()\n            global_step += 1\n            if global_step % args.gradient_accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n    \n        model.eval()\n        metrics, classACC, tgts, preds = model_eval(val_loader, model, args, criterion, device)\n        # logger.info(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n        # log_metrics(\"Val\", metrics, args, logger)\n    \n        tuning_metric = (\n            metrics[\"micro_f1\"] if args.task_type == \"multilabel\" else metrics[\"acc\"]\n        )\n        scheduler.step(tuning_metric)\n        is_improvement = tuning_metric > best_metric\n        if is_improvement:\n            best_metric = tuning_metric\n            n_no_improve = 0\n            torch.save(model.state_dict(), os.path.join(args.savedir, \"best_model.pth\"))\n        else:\n            n_no_improve += 1\n    \n        csv_save_name = args.save_name\n        save_path = args.savedir + '/' + csv_save_name + '.csv'\n        f = open(save_path, 'w', encoding='utf-8')\n        wr = csv.writer(f)\n        key = list(classACC.keys())\n        val = list(classACC.values())\n        title = ['micro_auc', 'macro_auc', 'micro_f1', 'macro_f1'] + key\n        result = [metrics[\"micro_roc_auc\"], metrics[\"macro_roc_auc\"], metrics[\"micro_f1\"], metrics[\"macro_f1\"]] + val\n        wr.writerow(title)\n        wr.writerow(result)\n        f.close()\n    \n        # save_checkpoint(\n        #     {\n        #         \"epoch\": i_epoch + 1,\n        #         \"state_dict\": model.state_dict(),\n        #         \"optimizer\": optimizer.state_dict(),\n        #         \"scheduler\": scheduler.state_dict(),\n        #         \"n_no_improve\": n_no_improve,\n        #         \"best_metric\": best_metric,\n        #     },\n        #     is_improvement,\n        #     args.savedir,\n        # )\n    \n        if n_no_improve >= args.patience:\n            logger.info(\"No improvement. Breaking out of loop.\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T18:30:54.773823Z","iopub.execute_input":"2025-02-20T18:30:54.774172Z","iopub.status.idle":"2025-02-20T18:30:54.785515Z","shell.execute_reply.started":"2025-02-20T18:30:54.774123Z","shell.execute_reply":"2025-02-20T18:30:54.784686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test(args):\n\n    print(\"Model Test\")\n    print(\" # PID :\", os.getpid())\n    print('log:', args.Valid_dset_name)\n    set_seed(args.seed)\n    args.savedir = os.path.join(args.savedir, os.name)\n    os.makedirs(args.savedir, exist_ok=True)\n\n    train_loader, val_loader = get_data_loaders(args)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = get_model(args)\n\n    criterion = get_criterion(args, device)\n\n    torch.save(args, os.path.join(args.savedir, \"args.bin\"))\n\n\n    if os.path.exists(os.path.join(args.loaddir, \"model_best.pt\")):\n        model.load_state_dict(torch.load(args.loaddir + \"/model_best.pt\"), strict=False)\n\n    else:\n        print(\"\")\n        print(\"\")\n        print(\"this option initilize the model with random value. train from scratch.\")\n        print(\"Loaded model : \")\n\n    print(\"freeze image?\", args.freeze_img_all)\n    print(\"freeze txt?\", args.freeze_txt_all)\n    model.to(device)\n\n    if torch.cuda.device_count() > 1:\n        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n        model = nn.DataParallel(model)\n\n    load_checkpoint(model, os.path.join(args.loaddir, \"model_best.pt\"))\n\n    model.eval()\n    metrics, classACC, tgts, preds  = model_eval(val_loader, model, args, criterion, device, store_preds=True)\n\n    print('micro_roc_auc:', round(metrics[\"micro_roc_auc\"], 3))\n    print('macro_roc_auc:', round(metrics[\"macro_roc_auc\"], 3))\n    print('macro_f1 f1 scroe:', round(metrics[\"macro_f1\"], 3))\n    print('micro f1 score:', round(metrics[\"micro_f1\"], 3))\n    for i in classACC:\n        print(i, round(classACC[i], 3))","metadata":{"id":"wEnM-WoDHPcA","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:51:06.888675Z","iopub.execute_input":"2025-02-20T16:51:06.888963Z","iopub.status.idle":"2025-02-20T16:51:06.896810Z","shell.execute_reply.started":"2025-02-20T16:51:06.888941Z","shell.execute_reply":"2025-02-20T16:51:06.895950Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"train(args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T18:31:18.549920Z","iopub.execute_input":"2025-02-20T18:31:18.550252Z","iopub.status.idle":"2025-02-20T19:58:47.825567Z","shell.execute_reply.started":"2025-02-20T18:31:18.550225Z","shell.execute_reply":"2025-02-20T19:58:47.824760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}