{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d48c67c",
   "metadata": {
    "id": "wuGwvW0mUstb",
    "papermill": {
     "duration": 0.008335,
     "end_time": "2025-02-21T00:13:30.804018",
     "exception": false,
     "start_time": "2025-02-21T00:13:30.795683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **0. UTILS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530380ee",
   "metadata": {
    "papermill": {
     "duration": 0.007335,
     "end_time": "2025-02-21T00:13:30.819026",
     "exception": false,
     "start_time": "2025-02-21T00:13:30.811691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0.1 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7823b6d",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:30.834641Z",
     "iopub.status.busy": "2025-02-21T00:13:30.834397Z",
     "iopub.status.idle": "2025-02-21T00:13:34.268464Z",
     "shell.execute_reply": "2025-02-21T00:13:34.267794Z"
    },
    "papermill": {
     "duration": 3.443627,
     "end_time": "2025-02-21T00:13:34.270002",
     "exception": false,
     "start_time": "2025-02-21T00:13:30.826375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b23e09d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:34.288938Z",
     "iopub.status.busy": "2025-02-21T00:13:34.288574Z",
     "iopub.status.idle": "2025-02-21T00:13:34.292357Z",
     "shell.execute_reply": "2025-02-21T00:13:34.291717Z"
    },
    "papermill": {
     "duration": 0.013968,
     "end_time": "2025-02-21T00:13:34.293528",
     "exception": false,
     "start_time": "2025-02-21T00:13:34.279560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345c4c7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:34.309263Z",
     "iopub.status.busy": "2025-02-21T00:13:34.309061Z",
     "iopub.status.idle": "2025-02-21T00:13:34.312411Z",
     "shell.execute_reply": "2025-02-21T00:13:34.311851Z"
    },
    "id": "FGYrtHSaU7OE",
    "papermill": {
     "duration": 0.012441,
     "end_time": "2025-02-21T00:13:34.313599",
     "exception": false,
     "start_time": "2025-02-21T00:13:34.301158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\n",
    "    Copied from https://github.com/huggingface/pytorch-pretrained-BERT\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "282a20a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:34.329337Z",
     "iopub.status.busy": "2025-02-21T00:13:34.329135Z",
     "iopub.status.idle": "2025-02-21T00:13:34.332926Z",
     "shell.execute_reply": "2025-02-21T00:13:34.332325Z"
    },
    "papermill": {
     "duration": 0.013055,
     "end_time": "2025-02-21T00:13:34.334109",
     "exception": false,
     "start_time": "2025-02-21T00:13:34.321054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def numpy_seed(seed, *addl_seeds):\n",
    "    \"\"\"Context manager which seeds the NumPy PRNG with the specified seed and\n",
    "    restores the state afterward\"\"\"\n",
    "    if seed is None:\n",
    "        yield\n",
    "        return\n",
    "    if len(addl_seeds) > 0:\n",
    "        seed = int(hash((seed, *addl_seeds)) % 1e6)\n",
    "    state = np.random.get_state()\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        np.random.set_state(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff0616e",
   "metadata": {
    "papermill": {
     "duration": 0.007139,
     "end_time": "2025-02-21T00:13:34.348711",
     "exception": false,
     "start_time": "2025-02-21T00:13:34.341572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0.2 Initialize arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf0985",
   "metadata": {
    "papermill": {
     "duration": 0.007107,
     "end_time": "2025-02-21T00:13:34.363306",
     "exception": false,
     "start_time": "2025-02-21T00:13:34.356199",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> Khởi tạo đối số cố định thay vì nhập"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac62dca8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:34.378609Z",
     "iopub.status.busy": "2025-02-21T00:13:34.378414Z",
     "iopub.status.idle": "2025-02-21T00:13:34.383471Z",
     "shell.execute_reply": "2025-02-21T00:13:34.382901Z"
    },
    "papermill": {
     "duration": 0.014214,
     "end_time": "2025-02-21T00:13:34.384785",
     "exception": false,
     "start_time": "2025-02-21T00:13:34.370571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.seed = 123\n",
    "        self.batch_sz = 8\n",
    "        self.max_epochs = 20\n",
    "        self.task_type = \"multilabel\"\n",
    "        self.n_workers = 4\n",
    "        self.patience = 20\n",
    "        \n",
    "        output_path = 'output'\n",
    "        self.savedir = \"/kaggle/working\"\n",
    "        self.save_name = 'mimic_par'\n",
    "        \n",
    "        self.loaddir = '/kaggle/input/medvill-weight'\n",
    "        self.name = \"scenario_name\"\n",
    "        \n",
    "        self.openi = False\n",
    "        self.data_path = '/kaggle/input/thesis-mimic-cxr-2-0-0'\n",
    "        self.Train_dset_name = 'train.jsonl'\n",
    "        self.Valid_dset_name = 'valid.jsonl'\n",
    "\n",
    "        self.embed_sz = 768\n",
    "        self.hidden_sz = 768\n",
    "        self.bert_model = \"bert-base-uncased\"\n",
    "        self.init_model = \"bert-base-uncased\"\n",
    "        \n",
    "        self.drop_img_percent = 0.0\n",
    "        self.dropout = 0.1\n",
    "        \n",
    "        self.freeze_img = 0\n",
    "        self.freeze_txt = 0\n",
    "        \n",
    "        self.freeze_img_all = False\n",
    "        self.freeze_txt_all = False\n",
    "        \n",
    "        self.glove_path = \"/path/to/glove_embeds/glove.840B.300d.txt\"\n",
    "        self.gradient_accumulation_steps = 2\n",
    "        self.hidden = []\n",
    "        \n",
    "        self.img_embed_pool_type = \"avg\"\n",
    "        self.img_hidden_sz = 2048\n",
    "        self.include_bn = True\n",
    "        \n",
    "        self.lr = 1e-3\n",
    "        self.lr_factor = 0.75\n",
    "        self.lr_patience = 5\n",
    "        \n",
    "        self.max_seq_len = 512\n",
    "        self.num_image_embeds = 256\n",
    "        \n",
    "        self.warmup = 0.1\n",
    "        self.weight_classes = 1\n",
    "    \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32af5d36",
   "metadata": {
    "id": "1j81M9GQPJoQ",
    "papermill": {
     "duration": 0.0075,
     "end_time": "2025-02-21T00:13:34.399693",
     "exception": false,
     "start_time": "2025-02-21T00:13:34.392193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **1. DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f250403",
   "metadata": {
    "id": "JcYMmFuJUDyi",
    "papermill": {
     "duration": 0.007215,
     "end_time": "2025-02-21T00:13:34.414162",
     "exception": false,
     "start_time": "2025-02-21T00:13:34.406947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.1 Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd10613a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:34.429554Z",
     "iopub.status.busy": "2025-02-21T00:13:34.429355Z",
     "iopub.status.idle": "2025-02-21T00:13:34.434218Z",
     "shell.execute_reply": "2025-02-21T00:13:34.433597Z"
    },
    "id": "fW0euaVhPE64",
    "papermill": {
     "duration": 0.01378,
     "end_time": "2025-02-21T00:13:34.435329",
     "exception": false,
     "start_time": "2025-02-21T00:13:34.421549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Vocab(object):\n",
    "    \"\"\"\n",
    "    A vocabulary class that maps words to indices and vice versa.\n",
    "\n",
    "    - If `emptyInit=True`, initializes an empty vocabulary.\n",
    "    - If `emptyInit=False`, initializes with special tokens: [PAD], [UNK], [CLS], [SEP], [MASK].\n",
    "\n",
    "    Attributes:\n",
    "        stoi (dict): Maps words to indices.\n",
    "        itos (list): Maps indices to words.\n",
    "        vocab_sz (int): Size of the vocabulary.\n",
    "\n",
    "    Methods:\n",
    "        add(words): Adds new words to the vocabulary if they don't exist.\n",
    "    \"\"\"\n",
    "    def __init__(self, emptyInit=False):\n",
    "        if emptyInit:\n",
    "            self.stoi, self.itos, self.vocab_sz = {}, [], 0\n",
    "        else:\n",
    "            self.stoi = {\n",
    "                w: i\n",
    "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "            }\n",
    "            self.itos = [w for w in self.stoi]\n",
    "            self.vocab_sz = len(self.itos)\n",
    "\n",
    "    def add(self, words):\n",
    "        cnt = len(self.itos)\n",
    "        for w in words:\n",
    "            if w in self.stoi:\n",
    "                continue\n",
    "            self.stoi[w] = cnt\n",
    "            self.itos.append(w)\n",
    "            cnt += 1\n",
    "        self.vocab_sz = len(self.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d942ee39",
   "metadata": {
    "id": "M-NKJuanUG0R",
    "papermill": {
     "duration": 0.007155,
     "end_time": "2025-02-21T00:13:34.450135",
     "exception": false,
     "start_time": "2025-02-21T00:13:34.442980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.2 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ca8fe",
   "metadata": {
    "papermill": {
     "duration": 0.007434,
     "end_time": "2025-02-21T00:13:34.465146",
     "exception": false,
     "start_time": "2025-02-21T00:13:34.457712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> Set ImageFile.LOAD_TRUNCATED_IMAGES = True để bỏ qua những ảnh hư"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c24efb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:34.480583Z",
     "iopub.status.busy": "2025-02-21T00:13:34.480388Z",
     "iopub.status.idle": "2025-02-21T00:13:34.483435Z",
     "shell.execute_reply": "2025-02-21T00:13:34.482889Z"
    },
    "id": "IBHtevk7UIJb",
    "papermill": {
     "duration": 0.011902,
     "end_time": "2025-02-21T00:13:34.484511",
     "exception": false,
     "start_time": "2025-02-21T00:13:34.472609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# from utils.utils import truncate_seq_pair, numpy_seed\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b69f113",
   "metadata": {
    "papermill": {
     "duration": 0.007084,
     "end_time": "2025-02-21T00:13:34.499574",
     "exception": false,
     "start_time": "2025-02-21T00:13:34.492490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> Convert RGB để 3 chiều"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9939863",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:34.515174Z",
     "iopub.status.busy": "2025-02-21T00:13:34.514974Z",
     "iopub.status.idle": "2025-02-21T00:13:34.522640Z",
     "shell.execute_reply": "2025-02-21T00:13:34.522106Z"
    },
    "papermill": {
     "duration": 0.016836,
     "end_time": "2025-02-21T00:13:34.523885",
     "exception": false,
     "start_time": "2025-02-21T00:13:34.507049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class JsonlDataset(Dataset):\n",
    "    \"\"\"Dataset for processing .jsonl files containing text and image data.\n",
    "\n",
    "    - Tokenizes text, truncates it to max length, and converts tokens to indices.\n",
    "    - Handles optional image data, applying transformations if available.\n",
    "    - Supports multilabel classification by converting labels into tensors.\n",
    "\n",
    "    Attributes:\n",
    "        data (list): List of parsed JSON objects from the dataset file.\n",
    "        tokenizer (callable): Tokenizer function to process text.\n",
    "        vocab (Vocab): Vocabulary object for token-to-index conversion.\n",
    "        transforms (callable): Transformations applied to images.\n",
    "        n_classes (int): Number of output classes.\n",
    "        max_seq_len (int): Maximum sequence length after accounting for images.\n",
    "\n",
    "    Methods:\n",
    "        __len__(): Returns the number of samples.\n",
    "        __getitem__(index): Retrieves a sample, processing text, images, and labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, tokenizer, transforms, vocab, args):\n",
    "        self.data = [json.loads(l) for l in open(data_path)]\n",
    "        self.data_dir = os.path.dirname(data_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.args = args\n",
    "        self.vocab = vocab\n",
    "        self.n_classes = len(args.labels)\n",
    "        self.text_start_token =  [\"[SEP]\"]\n",
    "\n",
    "        # Drop images randomly for generalization\n",
    "        with numpy_seed(0):\n",
    "            for row in self.data:\n",
    "                if np.random.random() < args.drop_img_percent:\n",
    "                    row[\"img\"] = None\n",
    "\n",
    "        self.max_seq_len = args.max_seq_len\n",
    "        self.max_seq_len -= args.num_image_embeds\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = (\n",
    "            self.text_start_token\n",
    "            + self.tokenizer(self.data[index][\"text\"])[: (self.max_seq_len - 1)] \n",
    "            + self.text_start_token\n",
    "        )\n",
    "        segment = torch.zeros(len(sentence))\n",
    "        sentence = torch.LongTensor(\n",
    "            [\n",
    "                self.vocab.stoi[w] if w in self.vocab.stoi else self.vocab.stoi[\"[UNK]\"]\n",
    "                for w in sentence\n",
    "            ]\n",
    "        )\n",
    "        if self.args.task_type == \"multilabel\":\n",
    "            label = torch.zeros(self.n_classes)\n",
    "            if self.data[index][\"label\"] == '':\n",
    "                self.data[index][\"label\"] = \"'Others'\"\n",
    "            else:\n",
    "                pass\n",
    "            label[\n",
    "                [self.args.labels.index(tgt) for tgt in self.data[index][\"label\"]]\n",
    "            ] = 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        image = None\n",
    "        if self.data[index][\"img\"]:\n",
    "            image = Image.open(\n",
    "                os.path.join(self.data_dir, self.data[index][\"img\"])).convert(\"RGB\")\n",
    "        else:\n",
    "            image = Image.fromarray(128 * np.ones((512, 512, 3), dtype=np.uint8))\n",
    "        image = self.transforms(image)\n",
    "\n",
    "        # The first SEP is part of Image Token.\n",
    "        segment = segment[1:]\n",
    "        sentence = sentence[1:]\n",
    "        # The first segment (0) is of images.\n",
    "        segment += 1\n",
    "\n",
    "        return sentence, segment, image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1140ee",
   "metadata": {
    "id": "YqFuvx_DWdAI",
    "papermill": {
     "duration": 0.007087,
     "end_time": "2025-02-21T00:13:34.538181",
     "exception": false,
     "start_time": "2025-02-21T00:13:34.531094",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.3 Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6502e59d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:34.553304Z",
     "iopub.status.busy": "2025-02-21T00:13:34.553100Z",
     "iopub.status.idle": "2025-02-21T00:13:39.028599Z",
     "shell.execute_reply": "2025-02-21T00:13:39.027750Z"
    },
    "papermill": {
     "duration": 4.484743,
     "end_time": "2025-02-21T00:13:39.030130",
     "exception": false,
     "start_time": "2025-02-21T00:13:34.545387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-pretrained-bert\r\n",
      "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl.metadata (86 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (1.26.4)\r\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (1.36.13)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (4.67.1)\r\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2024.11.6)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2024.9.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\r\n",
      "Requirement already satisfied: botocore<1.37.0,>=1.36.13 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-pretrained-bert) (1.36.13)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\r\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-pretrained-bert) (0.11.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->pytorch-pretrained-bert) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->pytorch-pretrained-bert) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->pytorch-pretrained-bert) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->pytorch-pretrained-bert) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->pytorch-pretrained-bert) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->pytorch-pretrained-bert) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2025.1.31)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.37.0,>=1.36.13->boto3->pytorch-pretrained-bert) (2.9.0.post0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->pytorch-pretrained-bert) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->pytorch-pretrained-bert) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->pytorch-pretrained-bert) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->pytorch-pretrained-bert) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->pytorch-pretrained-bert) (2024.2.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.37.0,>=1.36.13->boto3->pytorch-pretrained-bert) (1.17.0)\r\n",
      "Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pytorch-pretrained-bert\r\n",
      "Successfully installed pytorch-pretrained-bert-0.6.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57719821",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:39.047553Z",
     "iopub.status.busy": "2025-02-21T00:13:39.047309Z",
     "iopub.status.idle": "2025-02-21T00:13:43.340456Z",
     "shell.execute_reply": "2025-02-21T00:13:43.339653Z"
    },
    "id": "OverYFy6Wf1F",
    "papermill": {
     "duration": 4.303467,
     "end_time": "2025-02-21T00:13:43.342019",
     "exception": false,
     "start_time": "2025-02-21T00:13:39.038552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from data.dataset import JsonlDataset\n",
    "# from data.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8465000f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:43.359995Z",
     "iopub.status.busy": "2025-02-21T00:13:43.359519Z",
     "iopub.status.idle": "2025-02-21T00:13:43.364636Z",
     "shell.execute_reply": "2025-02-21T00:13:43.363874Z"
    },
    "papermill": {
     "duration": 0.015525,
     "end_time": "2025-02-21T00:13:43.365944",
     "exception": false,
     "start_time": "2025-02-21T00:13:43.350419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transforms(args):\n",
    "    \"\"\"Returns a set of image transformations based on the dataset source.\n",
    "\n",
    "    - If args.openi is True, converts images to grayscale with 3 channels.\n",
    "    - Converts images to tensors and normalizes using ImageNet statistics.\n",
    "\n",
    "    Args:\n",
    "        args: An object containing configuration settings.\n",
    "\n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: A sequence of image transformations.\n",
    "    \"\"\"\n",
    "    # if args.openi:\n",
    "    #     return transforms.Compose(\n",
    "    #         [\n",
    "    #             transforms.Grayscale(num_output_channels=3),\n",
    "    #             transforms.ToTensor(),\n",
    "    #             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    #         ])\n",
    "    # else:\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "317c0424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:43.383959Z",
     "iopub.status.busy": "2025-02-21T00:13:43.383715Z",
     "iopub.status.idle": "2025-02-21T00:13:43.387675Z",
     "shell.execute_reply": "2025-02-21T00:13:43.386871Z"
    },
    "papermill": {
     "duration": 0.015334,
     "end_time": "2025-02-21T00:13:43.389221",
     "exception": false,
     "start_time": "2025-02-21T00:13:43.373887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_labels_and_frequencies(path):\n",
    "    \"\"\"Extracts unique labels and their frequencies from a JSONL file.\n",
    "\n",
    "    - Reads labels from the dataset.\n",
    "    - Splits multi-label entries and replaces empty labels with 'Others'.\n",
    "    - Counts occurrences of each label.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (List of unique labels, Counter object with label frequencies).\n",
    "    \"\"\"\n",
    "    label_freqs = Counter()\n",
    "    data_labels = [json.loads(line)[\"label\"] for line in open(path)]\n",
    "    if type(data_labels) == list:\n",
    "        # Change a little here to read our csv\n",
    "        for label_row in data_labels:\n",
    "            label_freqs.update(label_row)\n",
    "    else:\n",
    "        pass\n",
    "    return list(label_freqs.keys()), label_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf842109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:43.406505Z",
     "iopub.status.busy": "2025-02-21T00:13:43.406297Z",
     "iopub.status.idle": "2025-02-21T00:13:43.409611Z",
     "shell.execute_reply": "2025-02-21T00:13:43.409031Z"
    },
    "papermill": {
     "duration": 0.012752,
     "end_time": "2025-02-21T00:13:43.410638",
     "exception": false,
     "start_time": "2025-02-21T00:13:43.397886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vocab(args):\n",
    "    vocab = Vocab()\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "        args.bert_model, do_lower_case=True\n",
    "    )\n",
    "    vocab.stoi = bert_tokenizer.vocab\n",
    "    vocab.itos = bert_tokenizer.ids_to_tokens\n",
    "    vocab.vocab_sz = len(vocab.itos)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50e7bc06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:43.427092Z",
     "iopub.status.busy": "2025-02-21T00:13:43.426879Z",
     "iopub.status.idle": "2025-02-21T00:13:43.431962Z",
     "shell.execute_reply": "2025-02-21T00:13:43.431359Z"
    },
    "papermill": {
     "duration": 0.014529,
     "end_time": "2025-02-21T00:13:43.433014",
     "exception": false,
     "start_time": "2025-02-21T00:13:43.418485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch, args):\n",
    "    lens = [len(row[0]) for row in batch]\n",
    "    bsz, max_seq_len = len(batch), max(lens)\n",
    "\n",
    "    mask_tensor = torch.zeros(bsz, max_seq_len).long()\n",
    "    text_tensor = torch.zeros(bsz, max_seq_len).long()\n",
    "    segment_tensor = torch.zeros(bsz, max_seq_len).long()\n",
    "\n",
    "    img_tensor = None\n",
    "    img_tensor = torch.stack([row[2] for row in batch])\n",
    "\n",
    "    if args.task_type == \"multilabel\":\n",
    "        # Multilabel case\n",
    "        tgt_tensor = torch.stack([row[3] for row in batch])\n",
    "    else:\n",
    "        # Single Label case\n",
    "        tgt_tensor = torch.cat([row[3] for row in batch]).long()\n",
    "\n",
    "    for i_batch, (input_row, length) in enumerate(zip(batch, lens)):\n",
    "        tokens, segment = input_row[:2]\n",
    "        text_tensor[i_batch, :length] = tokens\n",
    "        segment_tensor[i_batch, :length] = segment\n",
    "        mask_tensor[i_batch, :length] = 1\n",
    "\n",
    "    return text_tensor, segment_tensor, mask_tensor, img_tensor, tgt_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "389311e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:43.449360Z",
     "iopub.status.busy": "2025-02-21T00:13:43.449154Z",
     "iopub.status.idle": "2025-02-21T00:13:43.453929Z",
     "shell.execute_reply": "2025-02-21T00:13:43.453333Z"
    },
    "papermill": {
     "duration": 0.014277,
     "end_time": "2025-02-21T00:13:43.455190",
     "exception": false,
     "start_time": "2025-02-21T00:13:43.440913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_loaders(args):\n",
    "    tokenizer = (\n",
    "        BertTokenizer.from_pretrained(args.bert_model, do_lower_case=True).tokenize)\n",
    "\n",
    "    transforms = get_transforms(args)\n",
    "\n",
    "    args.labels, args.label_freqs = get_labels_and_frequencies(\n",
    "        os.path.join(args.data_path, args.Train_dset_name)\n",
    "    )\n",
    "\n",
    "    vocab = get_vocab(args)\n",
    "    args.vocab = vocab\n",
    "    args.vocab_sz = vocab.vocab_sz\n",
    "    args.n_classes = len(args.labels)\n",
    "\n",
    "    train = JsonlDataset(\n",
    "        os.path.join(args.data_path, args.Train_dset_name),\n",
    "        tokenizer,\n",
    "        transforms,\n",
    "        vocab,\n",
    "        args,\n",
    "    )\n",
    "\n",
    "    args.train_data_len = len(train)\n",
    "\n",
    "    dev = JsonlDataset(\n",
    "        os.path.join(args.data_path, args.Valid_dset_name),\n",
    "        tokenizer,\n",
    "        transforms,\n",
    "        vocab,\n",
    "        args,\n",
    "    )\n",
    "\n",
    "    collate = functools.partial(collate_fn, args=args)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train,\n",
    "        batch_size=args.batch_sz,\n",
    "        shuffle=True,\n",
    "        num_workers=args.n_workers,\n",
    "        collate_fn=collate,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        dev,\n",
    "        batch_size=args.batch_sz,\n",
    "        shuffle=False,\n",
    "        num_workers=args.n_workers,\n",
    "        collate_fn=collate,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader  # , test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0596f6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:43.471549Z",
     "iopub.status.busy": "2025-02-21T00:13:43.471349Z",
     "iopub.status.idle": "2025-02-21T00:13:43.474126Z",
     "shell.execute_reply": "2025-02-21T00:13:43.473388Z"
    },
    "papermill": {
     "duration": 0.012235,
     "end_time": "2025-02-21T00:13:43.475282",
     "exception": false,
     "start_time": "2025-02-21T00:13:43.463047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_loader, val_loader = get_data_loaders(args)\n",
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6299d9ec",
   "metadata": {
    "id": "xs8qKTFTXaED",
    "papermill": {
     "duration": 0.00783,
     "end_time": "2025-02-21T00:13:43.490878",
     "exception": false,
     "start_time": "2025-02-21T00:13:43.483048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **2. MODELS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453782cd",
   "metadata": {
    "id": "L9zj_upyXoIn",
    "papermill": {
     "duration": 0.008949,
     "end_time": "2025-02-21T00:13:43.508468",
     "exception": false,
     "start_time": "2025-02-21T00:13:43.499519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.1 Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71e765c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:43.527076Z",
     "iopub.status.busy": "2025-02-21T00:13:43.526793Z",
     "iopub.status.idle": "2025-02-21T00:13:43.549911Z",
     "shell.execute_reply": "2025-02-21T00:13:43.549074Z"
    },
    "id": "JuOe7cvqXdkC",
    "papermill": {
     "duration": 0.033865,
     "end_time": "2025-02-21T00:13:43.551366",
     "exception": false,
     "start_time": "2025-02-21T00:13:43.517501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73627453",
   "metadata": {
    "id": "GM0e1bGFWyU0",
    "papermill": {
     "duration": 0.008476,
     "end_time": "2025-02-21T00:13:43.568785",
     "exception": false,
     "start_time": "2025-02-21T00:13:43.560309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Lấy đặc trưng của ảnh qua mô hình ResNet50.\n",
    "- Flattening các đặc trưng để chuẩn bị cho các bước xử lý sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c5d7e41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:43.585410Z",
     "iopub.status.busy": "2025-02-21T00:13:43.585187Z",
     "iopub.status.idle": "2025-02-21T00:13:43.590021Z",
     "shell.execute_reply": "2025-02-21T00:13:43.589241Z"
    },
    "id": "B2leO2vPAVD0",
    "papermill": {
     "duration": 0.014607,
     "end_time": "2025-02-21T00:13:43.591319",
     "exception": false,
     "start_time": "2025-02-21T00:13:43.576712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.args = args\n",
    "        model = torchvision.models.resnet50(pretrained=True)\n",
    "        modules = list(model.children())[:-2]\n",
    "        self.model = nn.Sequential(*modules)\n",
    "    '''\n",
    "        pool_func = (\n",
    "            nn.AdaptiveAvgPool2d\n",
    "            if args.img_embed_pool_type == \"avg\"\n",
    "            else nn.AdaptiveMaxPool2d\n",
    "        )\n",
    "\n",
    "        if args.num_image_embeds in [1, 2, 3, 5, 7]:\n",
    "            self.pool = pool_func((args.num_image_embeds, 1))\n",
    "        elif args.num_image_embeds == 4:\n",
    "            self.pool = pool_func((2, 2))\n",
    "        elif args.num_image_embeds == 6:\n",
    "            self.pool = pool_func((3, 2))\n",
    "        elif args.num_image_embeds == 8:\n",
    "            self.pool = pool_func((4, 2))\n",
    "        elif args.num_image_embeds == 9:\n",
    "            self.pool = pool_func((3, 3))\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        # Bx3x224x224 -> Bx2048x7x7 -> Bx2048xN -> BxNx2048\n",
    "\n",
    "        # out = self.pool(self.model(x))\n",
    "        # out = torch.flatten(out, start_dim=2)\n",
    "        # out = out.transpose(1, 2).contiguous()\n",
    "\n",
    "        out = self.model(x)\n",
    "        out = torch.flatten(out, start_dim=2) #out torch.Size([100, 2048, 3])\n",
    "        out = out.transpose(1, 2).contiguous() #out torch.Size([100, 3, 2048])\n",
    "\n",
    "        # print(\"out.size()\",out.size())\n",
    "        # input(\"STOP!!!\")\n",
    "\n",
    "\n",
    "        return out  # BxNx2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c7abf2",
   "metadata": {
    "id": "TRZDg1_oXzq1",
    "papermill": {
     "duration": 0.00765,
     "end_time": "2025-02-21T00:13:43.606994",
     "exception": false,
     "start_time": "2025-02-21T00:13:43.599344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fcb5cb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:43.625898Z",
     "iopub.status.busy": "2025-02-21T00:13:43.625654Z",
     "iopub.status.idle": "2025-02-21T00:13:57.873393Z",
     "shell.execute_reply": "2025-02-21T00:13:57.872452Z"
    },
    "id": "K2FSjYHQZKGB",
    "papermill": {
     "duration": 14.258176,
     "end_time": "2025-02-21T00:13:57.874944",
     "exception": false,
     "start_time": "2025-02-21T00:13:43.616768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertModel\n",
    "# from models.image import ImageEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ab943",
   "metadata": {
    "id": "yddVGTjG3KnX",
    "papermill": {
     "duration": 0.007975,
     "end_time": "2025-02-21T00:13:57.891258",
     "exception": false,
     "start_time": "2025-02-21T00:13:57.883283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Trước tiên, lớp chuyển đổi hình ảnh thành embeddings.\n",
    "- Tiếp theo, mã hóa các token `[CLS]` và `[SEP]` để bao gồm chúng vào chuỗi embeddings.\n",
    "- Cuối cùng, các embeddings của văn bản và hình ảnh được cộng với embeddings vị trí và loại token, sau đó chuẩn hóa và thực hiện dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cdeb10f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:57.908289Z",
     "iopub.status.busy": "2025-02-21T00:13:57.907758Z",
     "iopub.status.idle": "2025-02-21T00:13:57.914586Z",
     "shell.execute_reply": "2025-02-21T00:13:57.913950Z"
    },
    "id": "ANahuAfBxXEl",
    "papermill": {
     "duration": 0.016579,
     "end_time": "2025-02-21T00:13:57.915746",
     "exception": false,
     "start_time": "2025-02-21T00:13:57.899167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageBertEmbeddings(nn.Module):\n",
    "    def __init__(self, args, embeddings):\n",
    "        super(ImageBertEmbeddings, self).__init__()\n",
    "        self.args = args\n",
    "        self.img_embeddings = nn.Linear(args.img_hidden_sz, args.hidden_sz)\n",
    "        self.position_embeddings = embeddings.position_embeddings\n",
    "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
    "        self.word_embeddings = embeddings.word_embeddings\n",
    "        self.LayerNorm = embeddings.LayerNorm\n",
    "        self.dropout = nn.Dropout(p=args.dropout)\n",
    "\n",
    "    def forward(self, input_imgs, token_type_ids):\n",
    "        bsz = input_imgs.size(0)\n",
    "        seq_length = self.args.num_image_embeds + 2  # +2 for CLS and SEP Token\n",
    "\n",
    "        # CLS Token\n",
    "        cls_id = torch.LongTensor([self.args.vocab.stoi[\"[CLS]\"]]).cuda()\n",
    "        cls_id = cls_id.unsqueeze(0).expand(bsz, 1)\n",
    "        cls_token_embeds = self.word_embeddings(cls_id)\n",
    "\n",
    "        # SEP Token\n",
    "        sep_id = torch.LongTensor([self.args.vocab.stoi[\"[SEP]\"]]).cuda()\n",
    "        sep_id = sep_id.unsqueeze(0).expand(bsz, 1)\n",
    "        sep_token_embeds = self.word_embeddings(sep_id)\n",
    "\n",
    "        # Image embeddings\n",
    "        imgs_embeddings = self.img_embeddings(input_imgs)\n",
    "        token_embeddings = torch.cat(\n",
    "            [cls_token_embeds, imgs_embeddings, sep_token_embeds], dim=1)\n",
    "\n",
    "        # Position and token type embeddings\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long).cuda()\n",
    "        position_ids = position_ids.unsqueeze(0).expand(bsz, seq_length)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        # Final embeddings\n",
    "        # print('token_sz:', token_embeddings.size())\n",
    "        # print('position_sz:', position_embeddings.size())\n",
    "        # print('token_type:', token_type_embeddings.size())\n",
    "        embeddings = token_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2f1fa7",
   "metadata": {
    "id": "Nd-r2YYCGiF8",
    "papermill": {
     "duration": 0.007692,
     "end_time": "2025-02-21T00:13:57.931157",
     "exception": false,
     "start_time": "2025-02-21T00:13:57.923465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Tạo attention mask để xử lý cả văn bản và hình ảnh.\n",
    "- Trích xuất đặc trưng hình ảnh bằng ResNet50 (ImageEncoder).\n",
    "- Tạo embeddings cho hình ảnh và văn bản.\n",
    "- Ghép embeddings hình ảnh và văn bản thành một chuỗi đầu vào cho BERT.\n",
    "- Truyền dữ liệu qua BERT encoder để học các mối quan hệ giữa hình ảnh và văn bản.\n",
    "- Trả về embedding cuối cùng để sử dụng cho các tác vụ tiếp theo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e899fa6",
   "metadata": {
    "papermill": {
     "duration": 0.007785,
     "end_time": "2025-02-21T00:13:57.981238",
     "exception": false,
     "start_time": "2025-02-21T00:13:57.973453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> Sửa lại float 16 thành float 32 trong hàm forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b208d5c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:57.998102Z",
     "iopub.status.busy": "2025-02-21T00:13:57.997797Z",
     "iopub.status.idle": "2025-02-21T00:13:58.004376Z",
     "shell.execute_reply": "2025-02-21T00:13:58.003744Z"
    },
    "id": "udLQu76rxU-L",
    "papermill": {
     "duration": 0.016244,
     "end_time": "2025-02-21T00:13:58.005602",
     "exception": false,
     "start_time": "2025-02-21T00:13:57.989358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultimodalBertEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MultimodalBertEncoder, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        if args.init_model == \"bert-base-scratch\":\n",
    "            config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "            bert = BertModel(config)\n",
    "        else:\n",
    "            bert = BertModel.from_pretrained(args.init_model)\n",
    "        self.txt_embeddings = bert.embeddings\n",
    "        \n",
    "        self.img_embeddings = ImageBertEmbeddings(args, self.txt_embeddings)\n",
    "        self.img_encoder = ImageEncoder(args)\n",
    "        self.encoder = bert.encoder\n",
    "        self.pooler = bert.pooler\n",
    "        #self.clf = nn.Linear(args.hidden_sz, args.n_classes)\n",
    "\n",
    "    def forward(self, input_txt, attention_mask, segment, input_img):\n",
    "        bsz = input_txt.size(0)\n",
    "        \n",
    "        # Attention mask for both text and image\n",
    "        attention_mask = torch.cat(\n",
    "            [\n",
    "                torch.ones(bsz, self.args.num_image_embeds + 2).long().cuda(),\n",
    "                attention_mask,\n",
    "            ],\n",
    "            dim=1)\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        try:\n",
    "            extended_attention_mask = extended_attention_mask.to(\n",
    "                dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "        except StopIteration:\n",
    "            extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        # Image embedding\n",
    "        img_tok = (\n",
    "            torch.LongTensor(input_txt.size(0), self.args.num_image_embeds + 2)\n",
    "            .fill_(0)\n",
    "            .cuda())\n",
    "        img = self.img_encoder(input_img)  # BxNx3x224x224 -> BxNx2048\n",
    "\n",
    "        img_embed_out = self.img_embeddings(img, img_tok)\n",
    "        txt_embed_out = self.txt_embeddings(input_txt, segment)\n",
    "        encoder_input = torch.cat([img_embed_out, txt_embed_out], 1)  # Bx(TEXT+IMG)xHID\n",
    "        encoded_layers = self.encoder(encoder_input, extended_attention_mask)\n",
    "        \n",
    "        return self.pooler(encoded_layers[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1680e8b0",
   "metadata": {
    "id": "C3_UXbBsG3OI",
    "papermill": {
     "duration": 0.007615,
     "end_time": "2025-02-21T00:13:58.021476",
     "exception": false,
     "start_time": "2025-02-21T00:13:58.013861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Lấy đầu ra từ MultimodalBertEncoder.\n",
    "- Đưa đầu ra qua lớp phân loại (clf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04623daa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:58.037997Z",
     "iopub.status.busy": "2025-02-21T00:13:58.037736Z",
     "iopub.status.idle": "2025-02-21T00:13:58.041687Z",
     "shell.execute_reply": "2025-02-21T00:13:58.041066Z"
    },
    "id": "LyruNDrKxTH6",
    "papermill": {
     "duration": 0.013458,
     "end_time": "2025-02-21T00:13:58.042893",
     "exception": false,
     "start_time": "2025-02-21T00:13:58.029435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultimodalBertClf(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MultimodalBertClf, self).__init__()\n",
    "        self.args = args\n",
    "        self.enc = MultimodalBertEncoder(args)\n",
    "        self.clf = nn.Linear(args.hidden_sz, args.n_classes)\n",
    "\n",
    "    def forward(self, txt, mask, segment, img):\n",
    "        x = self.enc(txt, mask, segment, img)\n",
    "        return self.clf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23fe1b64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:58.059257Z",
     "iopub.status.busy": "2025-02-21T00:13:58.059054Z",
     "iopub.status.idle": "2025-02-21T00:13:58.062394Z",
     "shell.execute_reply": "2025-02-21T00:13:58.061663Z"
    },
    "papermill": {
     "duration": 0.012815,
     "end_time": "2025-02-21T00:13:58.063591",
     "exception": false,
     "start_time": "2025-02-21T00:13:58.050776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"model\": MultimodalBertClf,\n",
    "}\n",
    "\n",
    "def get_model(args):\n",
    "    return MODELS['model'](args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cfcb16",
   "metadata": {
    "id": "JWCONO7pcHrZ",
    "papermill": {
     "duration": 0.007882,
     "end_time": "2025-02-21T00:13:58.079499",
     "exception": false,
     "start_time": "2025-02-21T00:13:58.071617",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **3. Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b33d8e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:58.096327Z",
     "iopub.status.busy": "2025-02-21T00:13:58.096129Z",
     "iopub.status.idle": "2025-02-21T00:13:58.099497Z",
     "shell.execute_reply": "2025-02-21T00:13:58.098870Z"
    },
    "id": "ppanGIy1cK9h",
    "papermill": {
     "duration": 0.013117,
     "end_time": "2025-02-21T00:13:58.100694",
     "exception": false,
     "start_time": "2025-02-21T00:13:58.087577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "from pytorch_pretrained_bert import BertAdam\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "# from data.helpers import get_data_loaders\n",
    "# from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "047dacc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:58.117420Z",
     "iopub.status.busy": "2025-02-21T00:13:58.117184Z",
     "iopub.status.idle": "2025-02-21T00:13:58.121401Z",
     "shell.execute_reply": "2025-02-21T00:13:58.120772Z"
    },
    "id": "1Us8IVA5HgPE",
    "papermill": {
     "duration": 0.013765,
     "end_time": "2025-02-21T00:13:58.122561",
     "exception": false,
     "start_time": "2025-02-21T00:13:58.108796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_criterion(args, device):\n",
    "    if args.task_type == \"multilabel\":\n",
    "        if args.weight_classes:\n",
    "            freqs = [args.label_freqs[l] for l in args.labels]\n",
    "            negative = [args.train_data_len - l for l in freqs]\n",
    "            label_weights = (torch.FloatTensor(freqs) / torch.FloatTensor(negative)) ** -1\n",
    "            criterion = nn.BCEWithLogitsLoss(pos_weight=label_weights.to(device))\n",
    "        else:\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb1d1720",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:58.139096Z",
     "iopub.status.busy": "2025-02-21T00:13:58.138835Z",
     "iopub.status.idle": "2025-02-21T00:13:58.143364Z",
     "shell.execute_reply": "2025-02-21T00:13:58.142707Z"
    },
    "id": "Chj-b0kxHeau",
    "papermill": {
     "duration": 0.01421,
     "end_time": "2025-02-21T00:13:58.144567",
     "exception": false,
     "start_time": "2025-02-21T00:13:58.130357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimizer(model, args):\n",
    "    total_steps = (\n",
    "            args.train_data_len\n",
    "            / args.batch_sz\n",
    "            / args.gradient_accumulation_steps\n",
    "            * args.max_epochs)\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
    "        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0, }]\n",
    "    optimizer = BertAdam(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=args.lr,\n",
    "        warmup=args.warmup,\n",
    "        t_total=total_steps)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "568ebb78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:58.161159Z",
     "iopub.status.busy": "2025-02-21T00:13:58.160937Z",
     "iopub.status.idle": "2025-02-21T00:13:58.164249Z",
     "shell.execute_reply": "2025-02-21T00:13:58.163480Z"
    },
    "id": "9PZ_YJ1THcrO",
    "papermill": {
     "duration": 0.012852,
     "end_time": "2025-02-21T00:13:58.165396",
     "exception": false,
     "start_time": "2025-02-21T00:13:58.152544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, args):\n",
    "    return optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \"max\", patience=args.lr_patience, verbose=True, factor=args.lr_factor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05515020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:58.181906Z",
     "iopub.status.busy": "2025-02-21T00:13:58.181671Z",
     "iopub.status.idle": "2025-02-21T00:13:58.189798Z",
     "shell.execute_reply": "2025-02-21T00:13:58.189020Z"
    },
    "id": "onQVcO3sHaRy",
    "papermill": {
     "duration": 0.017689,
     "end_time": "2025-02-21T00:13:58.190998",
     "exception": false,
     "start_time": "2025-02-21T00:13:58.173309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_eval(data, model, args, criterion, device, store_preds=False):\n",
    "    with torch.no_grad():\n",
    "        losses, preds, preds_bool, tgts, outAUROC = [], [], [], [], []\n",
    "        for batch in data:\n",
    "            loss, out, tgt = model_forward(model, args, criterion, batch, device)\n",
    "            losses.append(loss.item())\n",
    "            if args.task_type == \"multilabel\":\n",
    "                pred_bool = torch.sigmoid(out).cpu().detach().numpy() > 0.5\n",
    "                pred = torch.sigmoid(out).cpu().detach().numpy()\n",
    "            else:pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
    "            preds.append(pred)\n",
    "            preds_bool.append(pred_bool)\n",
    "            tgt = tgt.cpu().detach().numpy()\n",
    "            tgts.append(tgt)\n",
    "\n",
    "    metrics = {\"loss\": np.mean(losses)}\n",
    "    classACC = dict()\n",
    "    if args.task_type == \"multilabel\":\n",
    "        tgts = np.vstack(tgts)\n",
    "        preds = np.vstack(preds)\n",
    "        preds_bool = np.vstack(preds_bool)\n",
    "\n",
    "        for i in range(args.n_classes):\n",
    "            try:\n",
    "                outAUROC.append(roc_auc_score(tgts[:, i], preds[:, i]))\n",
    "            except ValueError:\n",
    "                outAUROC.append(0)\n",
    "                pass\n",
    "        for i in range(0, len(outAUROC)):\n",
    "            assert args.n_classes == len(outAUROC)\n",
    "            classACC[args.labels[i]] = outAUROC[i]\n",
    "\n",
    "        metrics[\"micro_roc_auc\"] = roc_auc_score(tgts, preds, average=\"micro\")\n",
    "        metrics[\"macro_roc_auc\"] = roc_auc_score(tgts, preds, average=\"macro\")\n",
    "        metrics[\"macro_f1\"] = f1_score(tgts, preds_bool, average=\"macro\")\n",
    "        metrics[\"micro_f1\"] = f1_score(tgts, preds_bool, average=\"micro\")\n",
    "        print('micro_auc:', metrics[\"micro_roc_auc\"])\n",
    "        print('micro_f1:', metrics[\"micro_f1\"])\n",
    "        print('-----------------------------------------------------')\n",
    "    else:\n",
    "        tgts = [l for sl in tgts for l in sl]\n",
    "        preds = [l for sl in preds for l in sl]\n",
    "        metrics[\"acc\"] = accuracy_score(tgts, preds)\n",
    "\n",
    "    if store_preds:\n",
    "        store_preds_to_disk(tgts, preds, args)\n",
    "\n",
    "    return metrics, classACC, tgts, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0658a09c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:58.207394Z",
     "iopub.status.busy": "2025-02-21T00:13:58.207198Z",
     "iopub.status.idle": "2025-02-21T00:13:58.211429Z",
     "shell.execute_reply": "2025-02-21T00:13:58.210842Z"
    },
    "id": "0_gZlPl7HWqj",
    "papermill": {
     "duration": 0.013671,
     "end_time": "2025-02-21T00:13:58.212504",
     "exception": false,
     "start_time": "2025-02-21T00:13:58.198833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_forward(model, args, criterion, batch, device):\n",
    "    txt, segment, mask, img, tgt = batch\n",
    "    model.to(device)\n",
    "    if args.num_image_embeds > 0:\n",
    "        for param in model.module.enc.img_encoder.parameters():\n",
    "            param.requires_grad = args.freeze_img_all\n",
    "    for param in model.module.enc.encoder.parameters():\n",
    "        param.requires_grad = args.freeze_txt_all\n",
    "\n",
    "    txt, img = txt.to(device), img.to(device)\n",
    "    mask, segment = mask.to(device), segment.to(device)\n",
    "    out = model(txt, mask, segment, img)\n",
    "\n",
    "    tgt = tgt.to(device)\n",
    "    loss = criterion(out, tgt)\n",
    "    return loss, out, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4023358",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:58.228911Z",
     "iopub.status.busy": "2025-02-21T00:13:58.228673Z",
     "iopub.status.idle": "2025-02-21T00:13:58.238254Z",
     "shell.execute_reply": "2025-02-21T00:13:58.237654Z"
    },
    "papermill": {
     "duration": 0.019006,
     "end_time": "2025-02-21T00:13:58.239382",
     "exception": false,
     "start_time": "2025-02-21T00:13:58.220376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    print(\"Training start!!\")\n",
    "    print(\" # PID :\", os.getpid())\n",
    "    \n",
    "    set_seed(args.seed)\n",
    "    args.savedir = os.path.join(args.savedir, args.save_name)\n",
    "    os.makedirs(args.savedir, exist_ok=True)\n",
    "    \n",
    "    train_loader, val_loader = get_data_loaders(args)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = get_model(args)\n",
    "    \n",
    "    criterion = get_criterion(args, device)\n",
    "    optimizer = get_optimizer(model, args)\n",
    "    scheduler = get_scheduler(optimizer, args)\n",
    "    \n",
    "    # logger = create_logger(\"%s/logfile.log\" % args.savedir, args)\n",
    "    torch.save(args, os.path.join(args.savedir, \"args.bin\"))\n",
    "    \n",
    "    start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
    "    \n",
    "    if os.path.exists(os.path.join(args.loaddir, \"pytorch_model.bin\")):\n",
    "        model.load_state_dict(torch.load(args.loaddir + \"/pytorch_model.bin\"), strict=False)\n",
    "    \n",
    "        print(\"This would load the trained model, then fine-tune the model.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"this option initilize the model with random value. train from scratch.\")\n",
    "        print(\"Loaded model : \")\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"freeze image?\", args.freeze_img_all)\n",
    "    print(\"freeze txt?\", args.freeze_txt_all)\n",
    "    model.to(device)\n",
    "    # logger.info(\"Training..\")\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    for i_epoch in range(start_epoch, args.max_epochs):\n",
    "        train_losses = []\n",
    "        # model.module.train()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        for batch in tqdm(train_loader, total=len(train_loader)):\n",
    "            loss, out, target = model_forward(model, args, criterion, batch, device)\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "    \n",
    "            train_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            global_step += 1\n",
    "            if global_step % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "        model.eval()\n",
    "        metrics, classACC, tgts, preds = model_eval(val_loader, model, args, criterion, device)\n",
    "        # logger.info(\"Train Loss: {:.4f}\".format(np.mean(train_losses)))\n",
    "        # log_metrics(\"Val\", metrics, args, logger)\n",
    "    \n",
    "        tuning_metric = (\n",
    "            metrics[\"micro_f1\"] if args.task_type == \"multilabel\" else metrics[\"acc\"]\n",
    "        )\n",
    "        scheduler.step(tuning_metric)\n",
    "        is_improvement = tuning_metric > best_metric\n",
    "        if is_improvement:\n",
    "            best_metric = tuning_metric\n",
    "            n_no_improve = 0\n",
    "            torch.save(model.state_dict(), os.path.join(args.savedir, \"best_model.pth\"))\n",
    "        else:\n",
    "            n_no_improve += 1\n",
    "    \n",
    "        csv_save_name = args.save_name\n",
    "        save_path = args.savedir + '/' + csv_save_name + '.csv'\n",
    "        f = open(save_path, 'w', encoding='utf-8')\n",
    "        wr = csv.writer(f)\n",
    "        key = list(classACC.keys())\n",
    "        val = list(classACC.values())\n",
    "        title = ['micro_auc', 'macro_auc', 'micro_f1', 'macro_f1'] + key\n",
    "        result = [metrics[\"micro_roc_auc\"], metrics[\"macro_roc_auc\"], metrics[\"micro_f1\"], metrics[\"macro_f1\"]] + val\n",
    "        wr.writerow(title)\n",
    "        wr.writerow(result)\n",
    "        f.close()\n",
    "    \n",
    "        # save_checkpoint(\n",
    "        #     {\n",
    "        #         \"epoch\": i_epoch + 1,\n",
    "        #         \"state_dict\": model.state_dict(),\n",
    "        #         \"optimizer\": optimizer.state_dict(),\n",
    "        #         \"scheduler\": scheduler.state_dict(),\n",
    "        #         \"n_no_improve\": n_no_improve,\n",
    "        #         \"best_metric\": best_metric,\n",
    "        #     },\n",
    "        #     is_improvement,\n",
    "        #     args.savedir,\n",
    "        # )\n",
    "    \n",
    "        if n_no_improve >= args.patience:\n",
    "            logger.info(\"No improvement. Breaking out of loop.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71573dad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:58.255625Z",
     "iopub.status.busy": "2025-02-21T00:13:58.255429Z",
     "iopub.status.idle": "2025-02-21T00:13:58.261744Z",
     "shell.execute_reply": "2025-02-21T00:13:58.261152Z"
    },
    "id": "wEnM-WoDHPcA",
    "papermill": {
     "duration": 0.015723,
     "end_time": "2025-02-21T00:13:58.262963",
     "exception": false,
     "start_time": "2025-02-21T00:13:58.247240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(args):\n",
    "\n",
    "    print(\"Model Test\")\n",
    "    print(\" # PID :\", os.getpid())\n",
    "    print('log:', args.Valid_dset_name)\n",
    "    set_seed(args.seed)\n",
    "    args.savedir = os.path.join(args.savedir, os.name)\n",
    "    os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "    train_loader, val_loader = get_data_loaders(args)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = get_model(args)\n",
    "\n",
    "    criterion = get_criterion(args, device)\n",
    "\n",
    "    torch.save(args, os.path.join(args.savedir, \"args.bin\"))\n",
    "\n",
    "\n",
    "    if os.path.exists(os.path.join(args.loaddir, \"model_best.pt\")):\n",
    "        model.load_state_dict(torch.load(args.loaddir + \"/model_best.pt\"), strict=False)\n",
    "\n",
    "    else:\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"this option initilize the model with random value. train from scratch.\")\n",
    "        print(\"Loaded model : \")\n",
    "\n",
    "    print(\"freeze image?\", args.freeze_img_all)\n",
    "    print(\"freeze txt?\", args.freeze_txt_all)\n",
    "    model.to(device)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    load_checkpoint(model, os.path.join(args.loaddir, \"model_best.pt\"))\n",
    "\n",
    "    model.eval()\n",
    "    metrics, classACC, tgts, preds  = model_eval(val_loader, model, args, criterion, device, store_preds=True)\n",
    "\n",
    "    print('micro_roc_auc:', round(metrics[\"micro_roc_auc\"], 3))\n",
    "    print('macro_roc_auc:', round(metrics[\"macro_roc_auc\"], 3))\n",
    "    print('macro_f1 f1 scroe:', round(metrics[\"macro_f1\"], 3))\n",
    "    print('micro f1 score:', round(metrics[\"micro_f1\"], 3))\n",
    "    for i in classACC:\n",
    "        print(i, round(classACC[i], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02993d4",
   "metadata": {
    "papermill": {
     "duration": 0.007783,
     "end_time": "2025-02-21T00:13:58.279069",
     "exception": false,
     "start_time": "2025-02-21T00:13:58.271286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7e32ba2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-21T00:13:58.295686Z",
     "iopub.status.busy": "2025-02-21T00:13:58.295463Z",
     "iopub.status.idle": "2025-02-21T01:38:57.550767Z",
     "shell.execute_reply": "2025-02-21T01:38:57.549832Z"
    },
    "papermill": {
     "duration": 5099.711426,
     "end_time": "2025-02-21T01:38:57.998180",
     "exception": false,
     "start_time": "2025-02-21T00:13:58.286754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start!!\n",
      " # PID : 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 3092483.03B/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29aa3deeac1442c7a2fadd406e494222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b4d41f741844088b41bebbf0f1fee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 179MB/s]\n",
      "<ipython-input-30-eb57d6081409>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(args.loaddir + \"/pytorch_model.bin\"), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This would load the trained model, then fine-tune the model.\n",
      "freeze image? False\n",
      "freeze txt? False\n",
      "Let's use 2 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [00:02<24:42,  2.97s/it]/usr/local/lib/python3.10/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1642.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "100%|██████████| 500/500 [03:38<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9294495547725498\n",
      "micro_f1: 0.7061604783757475\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9366621835167623\n",
      "micro_f1: 0.7282214813741121\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9518435412802061\n",
      "micro_f1: 0.7783310698657009\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9500559797987835\n",
      "micro_f1: 0.7815694291555276\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9539481921104849\n",
      "micro_f1: 0.782622187742436\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9525279494919856\n",
      "micro_f1: 0.7950290510006456\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9523443910790392\n",
      "micro_f1: 0.7953909465020578\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9533884873705099\n",
      "micro_f1: 0.7890267798824298\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9519710421878186\n",
      "micro_f1: 0.7947686116700201\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:41<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9512781484207542\n",
      "micro_f1: 0.7933665008291874\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9498532589505513\n",
      "micro_f1: 0.7946353730092205\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9487708222471988\n",
      "micro_f1: 0.7955377955377956\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9520833126115865\n",
      "micro_f1: 0.7964022894521667\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9513993240151781\n",
      "micro_f1: 0.7941663904540934\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9509322610243839\n",
      "micro_f1: 0.794228562059354\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9519207815911319\n",
      "micro_f1: 0.794766966475879\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9514260084445263\n",
      "micro_f1: 0.7914936035886361\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9510041551246537\n",
      "micro_f1: 0.7949400798934754\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9511840302471193\n",
      "micro_f1: 0.7934382323401405\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:42<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auc: 0.9514329864927368\n",
      "micro_f1: 0.7947086403215003\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f238bf",
   "metadata": {
    "papermill": {
     "duration": 0.441656,
     "end_time": "2025-02-21T01:38:58.942553",
     "exception": false,
     "start_time": "2025-02-21T01:38:58.500897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wuGwvW0mUstb",
    "1j81M9GQPJoQ",
    "JcYMmFuJUDyi",
    "M-NKJuanUG0R",
    "YqFuvx_DWdAI",
    "L9zj_upyXoIn"
   ],
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6703776,
     "sourceId": 10801109,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6649102,
     "sourceId": 10806635,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5134.855825,
   "end_time": "2025-02-21T01:39:03.061976",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-21T00:13:28.206151",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0cab608636384c4292c05ff9883e601c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "12d8b23212944de786e37aa90e55b668": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "13c27a081f454a87b21ef3e0ed053df6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0cab608636384c4292c05ff9883e601c",
       "placeholder": "​",
       "style": "IPY_MODEL_bcaba772c86f4af6a7ab2b052c1b667b",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: 100%"
      }
     },
     "22b4d41f741844088b41bebbf0f1fee5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ddb7af9a17bb4a0d9b0678018334e98a",
        "IPY_MODEL_fbfd563ddf5f47a5a3ba95ef2fd4fc40",
        "IPY_MODEL_563c442ab6634a10b01a3b6b3cf7663b"
       ],
       "layout": "IPY_MODEL_f9592c95a6aa4406b887a181ec69c648",
       "tabbable": null,
       "tooltip": null
      }
     },
     "29aa3deeac1442c7a2fadd406e494222": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_13c27a081f454a87b21ef3e0ed053df6",
        "IPY_MODEL_2a25f63c8b5e441da4e4c0910fce7395",
        "IPY_MODEL_3d0512691ff148fb9a7c5b5c21d3495d"
       ],
       "layout": "IPY_MODEL_12d8b23212944de786e37aa90e55b668",
       "tabbable": null,
       "tooltip": null
      }
     },
     "2a25f63c8b5e441da4e4c0910fce7395": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c9a13d22a77145719a0b236274110b1b",
       "max": 570.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dc1fdf0400de4c9085228a6887561e3d",
       "tabbable": null,
       "tooltip": null,
       "value": 570.0
      }
     },
     "3d0512691ff148fb9a7c5b5c21d3495d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5599e40f05cb45f2af1490cabd2fd302",
       "placeholder": "​",
       "style": "IPY_MODEL_7dc2f4a8b5e144a0b6672d1793ab8af0",
       "tabbable": null,
       "tooltip": null,
       "value": " 570/570 [00:00&lt;00:00, 51.1kB/s]"
      }
     },
     "4599b3b1251a4f9da517b0765bbb8376": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5599e40f05cb45f2af1490cabd2fd302": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "563c442ab6634a10b01a3b6b3cf7663b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6dca36f161144f4dba0c4fb748f06553",
       "placeholder": "​",
       "style": "IPY_MODEL_4599b3b1251a4f9da517b0765bbb8376",
       "tabbable": null,
       "tooltip": null,
       "value": " 440M/440M [00:02&lt;00:00, 217MB/s]"
      }
     },
     "6dca36f161144f4dba0c4fb748f06553": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7dc2f4a8b5e144a0b6672d1793ab8af0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a52f51d5ab4342b88d5d112bfaed8600": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bcaba772c86f4af6a7ab2b052c1b667b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c9a13d22a77145719a0b236274110b1b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cf321d7bea634a76a68bb49d1c80480d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dc1fdf0400de4c9085228a6887561e3d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dd599818e6584c648a7564f0bca6a7f8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ddb7af9a17bb4a0d9b0678018334e98a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cf321d7bea634a76a68bb49d1c80480d",
       "placeholder": "​",
       "style": "IPY_MODEL_a52f51d5ab4342b88d5d112bfaed8600",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "f8b6a582d570472d8bbdea89d21fc96a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f9592c95a6aa4406b887a181ec69c648": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fbfd563ddf5f47a5a3ba95ef2fd4fc40": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_dd599818e6584c648a7564f0bca6a7f8",
       "max": 440449768.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f8b6a582d570472d8bbdea89d21fc96a",
       "tabbable": null,
       "tooltip": null,
       "value": 440449768.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
